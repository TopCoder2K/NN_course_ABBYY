{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33a7d46-d93f-40b0-8256-97a1c43344e6",
   "metadata": {},
   "source": [
    "## Домашнее задание по свёрточным сетям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbc722-2c7e-41a1-b25f-913c32228a2d",
   "metadata": {},
   "source": [
    "Сутью домашнего задания является последовательная реализация базовых операций, применяемых в свёрточных сетях с использованием операций над тензорами PyTorch, но без применения модулей torch.nn. Студенты должны самостоятельно реализовать как прямой и обратный проходы слоёв, так и классы нейронных сетей.\n",
    "\n",
    "Правильность выполнения задания будет проверяться идентичностью прохождения процесса обучения в тех же архитектурах, выполненных с применением модулей и алгоритмов PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af498e6-0c14-4731-a899-ad5f6b19766b",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78d40c-6d89-49d5-be7f-43f9e8607921",
   "metadata": {},
   "source": [
    "В первом задании требуется реализовать классы двухмерной свёртки **Conv**, линейного слоя **Fc**, алгоритм обучения нейронной сети **SGD**, функцию активации **ReLU**, **Softmax** и функцию эмпирического риска **CrossEntropyLoss**.\n",
    "\n",
    "Свёрточные и полносвязные слои должны реализовывать операцию сдвига (*bias*, *b*). \n",
    "Сверить формулы прямого прохода можно в документации по [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) и [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Далее, следует реализовать класс модели, который включает в себя реализованные выше компоненты. Требуется повторить параметры обучения и архитектуру, реализованную ниже с помощью torch.nn модулей. Критерием правильности решения будет совпадение значений эмпирического риска при обучении обеих реализаций сетей на одних и тех же данных, с теми же параметрами и с одинаковыми начальными инициализациями весов.\n",
    "\n",
    "Данные для обучения состоят из 4 примеров вертикальных и горизонтальных линий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a74f3a-578c-4ca9-aa1c-1834e19b7bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:38.117158Z",
     "start_time": "2021-05-11T09:25:37.237134Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "\n",
    "_ = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a852825-7062-4829-a9e3-3e341cc46076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:38.123272Z",
     "start_time": "2021-05-11T09:25:38.118772Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y, num_classes):\n",
    "    N = y.shape[0]\n",
    "    Z = torch.zeros((N, num_classes))\n",
    "    Z[torch.arange(N), y] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b94731e-4897-4ad1-96a5-b128216b6922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:38.829791Z",
     "start_time": "2021-05-11T09:25:38.126317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape [Batch, Channels, Height, Width] = torch.Size([4, 1, 5, 5])\n",
      "Labels shape = torch.Size([4])\n",
      "Mean and standard deviation before normalization = 49.90, 97.45\n",
      "Mean and standard deviation after normalization = 0.00, 1.00\n",
      "Train batch\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Обучение будет вестись на следующих данных\n",
    "def get_data():\n",
    "    # Задача классификации вертикальных и горизонтальных линий\n",
    "    # Данные\n",
    "    vert1 = [[0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0]]\n",
    "\n",
    "    vert2 = [[0, 0, 220, 40, 0],\n",
    "             [0, 0, 250, 10, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 10, 250, 0, 0],\n",
    "             [0, 40, 220, 0, 0]]\n",
    "\n",
    "    hor1 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [250, 250, 250, 250, 250],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    hor2 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 20],\n",
    "            [220, 250, 250, 250, 200],\n",
    "            [10, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    data = [vert1, vert2, hor1, hor2]\n",
    "    labels = [0, 0, 1, 1]\n",
    "    \n",
    "    train_x = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return train_x, labels\n",
    "\n",
    "train_x, labels = get_data()\n",
    "print(f'Train data shape [Batch, Channels, Height, Width] = {train_x.shape}')\n",
    "print(f'Labels shape = {labels.shape}')\n",
    "\n",
    "\n",
    "train_mean = torch.mean(train_x)\n",
    "train_std = torch.std(train_x)\n",
    "batch = (train_x - train_mean) / train_std\n",
    "\n",
    "print(f'Mean and standard deviation before normalization = {train_mean.item():.2f}, {train_std.item():.2f}')\n",
    "print(f'Mean and standard deviation after normalization = {torch.mean(batch).item():.2f}, {torch.std(batch).item():.2f}')\n",
    "\n",
    "print('Train batch')\n",
    "for img in batch:\n",
    "    plt.imshow(img.squeeze(0), cmap='Greys')  # Цвета инвертированы. Чем темнее, тем значение пикселя больше\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2d43-d5d9-46cb-92f4-1b55e8870dbe",
   "metadata": {},
   "source": [
    "Реализуем эталонную модель **TorchGradientModel**, состоящую из следующих модулей:\n",
    "- Сверточный слой с 4 фильтрами размера $5\\times5$;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Линейный слой с 2 выходными нейронами для классов горизонтальной и вертикальной линий\n",
    "\n",
    "Вопрос:\n",
    "Каким ещё образом можно осуществить бинарную классификацию, не используя линейный слой, с 2 выходными нейронами?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f54ef4-1d73-4e63-ba2b-f904df6534cf",
   "metadata": {},
   "source": [
    "**Ответ.** Можно подобрать размер свёрток так, чтобы на выходе получалось два канала, в каждом из которых будет тензор вероятности принадлежности к классам (в нашем случае детекция не нужна, поэтому достаточно на выходе получить 2 тензора размера 1 на 1, то есть output.shape = 2x1x1).\n",
    "\n",
    "А ещё можно попробовать с одним нейроном: применять какую-нибудь сигмоиду и смотреть, если значение > 0.5, то отправляем в класс 1, иначе --- в класс 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da78879f-cbf9-41d5-98d3-806367c5b86a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:38.837205Z",
     "start_time": "2021-05-11T09:25:38.832388Z"
    }
   },
   "outputs": [],
   "source": [
    "class TorchGradientModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, padding=0, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * 1 * 1, 2)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c48edb-7561-46c0-a045-f739adb76b97",
   "metadata": {},
   "source": [
    "Дополнительная информация может быть найдена в комментариях к коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6dedb7-e1ce-4283-bd3c-650e3d5aca03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.085823Z",
     "start_time": "2021-05-11T09:25:38.839210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Данная сеть обучается за 3 эпохи, что удобно для процесса отладки.\n",
    "learning_rate = 1\n",
    "epochs = 3\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "# torch_model_params[0] - тензор с весами фильтров W свёрточного слоя\n",
    "# torch_model_params[1] - тензор с весами сдвигов b свёрточного слоя\n",
    "# torch_model_params[2] - тензор с весами W линейного слоя\n",
    "# torch_model_params[3] - тензор с весами сдвигов b линейного слоя\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета \n",
    "# (для I-го слоя - это кол-во изображений),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для данного эксперимента используется самый простой алгоритм обучения без моментов.\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, \n",
    "                            momentum=0, dampening=0, weight_decay=0, \n",
    "                            nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1425e7-8003-42e7-bac1-7f13d7852a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.262343Z",
     "start_time": "2021-05-11T09:25:39.097011Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 50.95it/s, accuracy=1, loss=0.0179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
      "          [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
      "          [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
      "          [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
      "          [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
      "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
      "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
      "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
      "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
      "\n",
      "\n",
      "        [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
      "          [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
      "          [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
      "          [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
      "          [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
      "          [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
      "          [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
      "          [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
      "          [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.1832, -0.0370,  0.1128,  0.0866], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.3232, -0.4252,  0.4799,  0.0261],\n",
      "        [ 0.3427,  0.1036,  0.1608,  0.3735]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4741, -0.3318], requires_grad=True))\n",
      "==============================================================\n",
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.0790, -0.1124, -0.4897,  0.0344, -0.2124],\n",
      "          [ 0.0958, -0.0652, -0.3758, -0.0051, -0.0255],\n",
      "          [ 0.5708,  0.5599,  0.1697,  0.4721,  0.4777],\n",
      "          [-0.0305, -0.0040, -0.4783,  0.1507,  0.0381],\n",
      "          [-0.0986, -0.1803, -0.4845, -0.1104, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0814,  0.1911, -0.1584, -0.1517,  0.1959],\n",
      "          [ 0.1277,  0.2616, -0.1562, -0.1174, -0.1183],\n",
      "          [-0.0246,  0.2475, -0.3215,  0.1575, -0.0211],\n",
      "          [ 0.2681, -0.0198, -0.1382,  0.0742, -0.0335],\n",
      "          [ 0.1747, -0.0624, -0.2083,  0.0141,  0.0499]]],\n",
      "\n",
      "\n",
      "        [[[-0.1807, -0.0838,  0.3246, -0.0201,  0.2108],\n",
      "          [ 0.1719,  0.0022,  0.0953,  0.1376,  0.0625],\n",
      "          [-0.3763, -0.1337,  0.0751, -0.0862, -0.3480],\n",
      "          [-0.0269,  0.0119,  0.2580, -0.1402, -0.1304],\n",
      "          [ 0.1204, -0.0184,  0.3138,  0.0230,  0.0772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0601, -0.1361, -0.2162,  0.0881],\n",
      "          [-0.1532, -0.1635, -0.2369, -0.0513,  0.1170],\n",
      "          [ 0.1530,  0.1091,  0.1960,  0.2573,  0.3693],\n",
      "          [ 0.0578, -0.1672, -0.2089,  0.0355, -0.1465],\n",
      "          [-0.1075,  0.0372, -0.1994, -0.0976, -0.0731]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.2302, -0.1774,  0.0902,  0.1112], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.3022, -0.2247,  0.4435,  0.2730],\n",
      "        [ 0.3217, -0.0969,  0.1973,  0.1266]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4034, -0.2611], requires_grad=True))\n",
      "==============================================================\n",
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 6.7224e-02, -1.2410e-01, -5.0147e-01,  2.2663e-02, -2.2412e-01],\n",
      "          [ 8.4114e-02, -7.6974e-02, -3.8749e-01, -1.6871e-02, -3.4586e-02],\n",
      "          [ 6.1396e-01,  6.0695e-01,  2.1672e-01,  5.1914e-01,  5.1817e-01],\n",
      "          [-4.0968e-02, -1.5686e-02, -4.9003e-01,  1.3900e-01,  2.6409e-02],\n",
      "          [-1.1031e-01, -1.9200e-01, -4.9622e-01, -1.2210e-01, -9.9919e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1444e-02,  1.9109e-01, -1.5841e-01, -1.5174e-01,  1.9585e-01],\n",
      "          [ 1.2774e-01,  2.6158e-01, -1.5618e-01, -1.1738e-01, -1.1829e-01],\n",
      "          [-2.4599e-02,  2.4749e-01, -3.2151e-01,  1.5746e-01, -2.1074e-02],\n",
      "          [ 2.6811e-01, -1.9776e-02, -1.3821e-01,  7.4236e-02, -3.3496e-02],\n",
      "          [ 1.7467e-01, -6.2394e-02, -2.0830e-01,  1.4116e-02,  4.9941e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9430e-01, -9.7377e-02,  3.7490e-01, -2.8138e-02,  1.9719e-01],\n",
      "          [ 1.5830e-01, -1.1385e-02,  1.4975e-01,  1.2540e-01,  4.8969e-02],\n",
      "          [-3.8983e-01, -1.4726e-01,  1.2952e-01, -9.9788e-02, -3.6157e-01],\n",
      "          [-4.0482e-02, -2.7019e-04,  3.1246e-01, -1.5380e-01, -1.4394e-01],\n",
      "          [ 1.0678e-01, -2.6421e-02,  3.6407e-01,  9.3816e-03,  6.3649e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4158e-02,  6.2900e-02, -1.3335e-01, -2.1350e-01,  9.0850e-02],\n",
      "          [-1.5044e-01, -1.6072e-01, -2.3417e-01, -4.8516e-02,  1.1910e-01],\n",
      "          [ 1.4289e-01,  9.8093e-02,  1.8493e-01,  2.4623e-01,  3.5975e-01],\n",
      "          [ 6.0202e-02, -1.6441e-01, -2.0615e-01,  3.8219e-02, -1.4373e-01],\n",
      "          [-1.0471e-01,  3.9919e-02, -1.9666e-01, -9.4895e-02, -7.0363e-02]]]],\n",
      "       requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.2531, -0.1774,  0.1167,  0.1058], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.5160, -0.2247,  0.7183,  0.1626],\n",
      "        [ 0.5355, -0.0969, -0.0775,  0.2370]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4744, -0.3320], requires_grad=True))\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "    # Можете выводить веса сети для прямого сравнения со своей реализацей\n",
    "    print('Parameters')\n",
    "    for param in torch_grad_model.named_parameters():\n",
    "        print(param)\n",
    "    print('==============================================================')\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты также можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\"\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605478d-3157-4556-a5af-3fae08b02da1",
   "metadata": {},
   "source": [
    "Ниже представлены заготовки (шаблоны) классов, колторые требуется реализовать. **Conv**, **Fc** и **ReLU** должны иметь методы **forward** и **backward** для прямого и обратного прохода по сети. При прямом проходе следует кэшировать данные, которые потребуются для вычисления градиента.\n",
    "\n",
    "**Свёртки**\n",
    "\n",
    "При прямом проходе нужно брать фильтры поочерёдно и проводить свёртку со входным тензором. Каждая операция свёртки даёт 2-мерную матрицу на выходе. Для получения итоговой карты признаков следует сконкатенировать эти матрицы, чтобы получить тензор рамерами [Размер батча, Количество каналов, Высота, Ширина]. Реализовывать можно как с помощью вложенных циклов, так и с применением векторизации. В данном задании важно не время работы, но точность вычислений.\n",
    "\n",
    "В первом модуле заданий уже была показана реализация обратного прохода по линейному слою для подсчёта частных производных по эмпирическому риску, которые использовались для обновления весов слоя. Расчёт частных производных в свёрточных слоях идеологически тот же. Требуется посчитать частные производные по dX предыдущему входу слоя, dW по весам фильтров и db по сдвигам. Пусть dZ – это градиент ошибки к выходу текущего свёрточного слоя (передаётся от предыдущего слоя при обратном проходе сети), тогда\n",
    "\n",
    "$dX += \\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W} W_c\\times dZ_{hw}$,\n",
    "\n",
    "где $W_c$ – это фильтр, а $dZ_{hw}$ – скаляр, соответствующий градиенту эмпирического риска к выходу текущего свёрточного слоя $Z$ в $n$ строке и $w$ столбце. Так как при прямом проходе фильтр $W_c$ влияет на все значения канала с карты признаков, то мы умножаем один и тот же фильтр $W_c$ с разными $dZ$ в пределах канала $с$, суммируя результаты.\n",
    "В numpy эта операция выглядела бы так\n",
    "\n",
    " dX[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    " \n",
    "В PyTorch очерёдность каналов иная: [Batch, Channel, Height, Width].\n",
    "\n",
    "Производная одного фильтра относительно эмпирического риска  считается по формуле\n",
    "\n",
    "$dW_c+=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}x_{slice} \\times dZ_{hw}$,\n",
    "\n",
    "где $x_{slice}$ относится к отрезку из входного тензора, который был использован в прямом проходе, чтобы получить активацию $Z_{ij}$. Таким образом мы получим градиент фильтра $W$ относительно данного отрезка. Так как в рамках свёрточного слоя для разных отрезков мы использовали тот же фильтр $W$, то мы складываем эти градиенты, чтобы получить $dW$.\n",
    "\n",
    "В numpy подобная операция реализуется так:\n",
    "\n",
    "dW[:,:,:,c] += x_slice * dZ[i, h, w, c]\n",
    "\n",
    "Производная по сдвигам считается как сумма всех градиентов выхода свёрточного слоя:\n",
    "\n",
    "$db=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}dZ_{hw}$\n",
    "\n",
    "В numpy реализовывалась бы так:\n",
    "\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "\n",
    "\n",
    "Для реализации выполнения обратного прохода градиента идентичным nn.CrossEntropyLoss(reduction='mean') образом, значения $dW$ и $db$ следует делить на размер пакета (batch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73eb63",
   "metadata": {},
   "source": [
    "**Замечание.** Хочется понять откуда взялись эти формулы... Вот [тут](https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509) и [тут](https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c) выводят с примерами, но без учёта `stride` и `padding`.\n",
    "\n",
    "Я сам пытался вывести часа 3 формулы в общем случае, но c градиентом по входу потрепел фиаско. Как же хорошо, что мне скинули в итоге вот эту [статью](https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327ef530-74d6-4ab3-b935-d5766410aee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.299203Z",
     "start_time": "2021-05-11T09:25:39.267428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task2Modules.CNN.Conv at 0x7fd2342ada90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Task2Modules.CNN import Conv\n",
    "\n",
    "Conv(1, 1, 1, init=torch_model_params)  # Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade385",
   "metadata": {},
   "source": [
    "**Замечание.** Дальше идут слои из первого задания, поэтому их просто скопируем оттуда ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f6aff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.415570Z",
     "start_time": "2021-05-11T09:25:39.305920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task2Modules.OtherLayers.Fc at 0x7fd23425bf50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Task2Modules.OtherLayers import Fc, Softmax, LogSoftmax, Flatten\n",
    "from Task2Modules.Optim import SGD\n",
    "from Task2Modules.Activations import ReLU\n",
    "from Task2Modules.Losses import CrossEntropyLoss\n",
    "\n",
    "Fc(5, 5, torch_model_params[2:4])  # Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b894b-cf32-40cf-84e0-d172f23436fe",
   "metadata": {},
   "source": [
    "Реализовать модель, идентичную **TorchGradientModel**. \n",
    "\n",
    "Если есть желание переиспользовать код из первого модуля заданий по линейным слоям, то можно реализовать модель, пользуясь предыдущими абстрактными классами и импортировать их здесь, переписав код для проверки результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7233e2-c485-45f0-b33f-3e3de37741c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.509328Z",
     "start_time": "2021-05-11T09:25:39.420602Z"
    }
   },
   "outputs": [],
   "source": [
    "from Task2Modules.CustomModels import CustomModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc89670-2027-4321-abbb-a3f5365054ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:39.617104Z",
     "start_time": "2021-05-11T09:25:39.513904Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_custom_model(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel(input_size=X.shape[-1], num_classes=num_classes, \n",
    "                        init=torch_model_params)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr=learning_rate, params=params)      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        print('Parameters')\n",
    "        for name, val in params.items():\n",
    "            print(name, val)\n",
    "        print('=============================================================')\n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        # TODO: зачем такой функционал... параметры ведь по ссылке передаются\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = (\n",
    "            torch.sum(torch.argmax(y_pred, axis=1) == labels).item() \\\n",
    "            / X.shape[0]\n",
    "        )\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba60e2ae-3472-4d16-ad2f-076dd35f839e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:25:42.633310Z",
     "start_time": "2021-05-11T09:25:42.370549Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  7.69it/s, acc=0.5, loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "W1 tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
      "          [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
      "          [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
      "          [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
      "          [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
      "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
      "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
      "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
      "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
      "\n",
      "\n",
      "        [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
      "          [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
      "          [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
      "          [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
      "          [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
      "          [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
      "          [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
      "          [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
      "          [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]]])\n",
      "b1 tensor([ 0.1832, -0.0370,  0.1128,  0.0866])\n",
      "W2 tensor([[-0.3232,  0.3427],\n",
      "        [-0.4252,  0.1036],\n",
      "        [ 0.4799,  0.1608],\n",
      "        [ 0.0261,  0.3735]])\n",
      "b2 tensor([ 0.4741, -0.3318])\n",
      "=============================================================\n",
      "Parameters\n",
      "W1 tensor([[[[ 0.0481, -0.1206, -0.0717,  0.0603, -0.2224],\n",
      "          [ 0.3535,  0.1573,  0.2419,  0.1097, -0.0009],\n",
      "          [ 0.5544,  0.4159,  0.3631,  0.0962,  0.0436],\n",
      "          [-0.5834, -0.4326, -0.3470, -0.0559, -0.0532],\n",
      "          [-0.3630, -0.3516, -0.2066, -0.2017, -0.1218]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
      "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
      "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
      "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
      "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
      "\n",
      "\n",
      "        [[[-0.1660, -0.0798,  0.1243, -0.0325,  0.2156],\n",
      "          [ 0.0484, -0.1044, -0.2007,  0.0826,  0.0508],\n",
      "          [-0.3684, -0.0647, -0.0176,  0.0939, -0.1400],\n",
      "          [ 0.2380,  0.2173,  0.1951, -0.0412, -0.0866],\n",
      "          [ 0.2470,  0.0637,  0.1806,  0.0667,  0.0933]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0153,  0.0558,  0.0820, -0.2027,  0.0829],\n",
      "          [-0.0188, -0.0474,  0.0853,  0.0086,  0.1298],\n",
      "          [ 0.1444,  0.0340,  0.2969,  0.0612,  0.1428],\n",
      "          [-0.2306, -0.3907, -0.1404, -0.0723, -0.1941],\n",
      "          [-0.2454, -0.0522, -0.0545, -0.1453, -0.0906]]]])\n",
      "b1 tensor([ 1.3598, -3.5457, -0.4511,  0.7003])\n",
      "W2 tensor([[-0.3022,  0.3217],\n",
      "        [-0.2247, -0.0969],\n",
      "        [ 0.4435,  0.1973],\n",
      "        [ 0.2730,  0.1266]])\n",
      "b2 tensor([ 0.4034, -0.2611])\n",
      "=============================================================\n",
      "Parameters\n",
      "W1 tensor([[[[ 0.0406, -0.1251, -0.0762,  0.0557, -0.2271],\n",
      "          [ 0.3855,  0.1845,  0.2611,  0.1209,  0.0023],\n",
      "          [ 0.6228,  0.4716,  0.4029,  0.1201,  0.0516],\n",
      "          [-0.6610, -0.4959, -0.3945, -0.0876, -0.0690],\n",
      "          [-0.4025, -0.3832, -0.2304, -0.2175, -0.1297]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
      "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
      "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
      "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
      "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
      "\n",
      "\n",
      "        [[[-0.1660, -0.0798,  0.1243, -0.0325,  0.2156],\n",
      "          [ 0.0484, -0.1044, -0.2007,  0.0826,  0.0508],\n",
      "          [-0.3684, -0.0647, -0.0176,  0.0939, -0.1400],\n",
      "          [ 0.2380,  0.2173,  0.1951, -0.0412, -0.0866],\n",
      "          [ 0.2470,  0.0637,  0.1806,  0.0667,  0.0933]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0171,  0.0569,  0.0830, -0.2016,  0.0840],\n",
      "          [-0.0263, -0.0538,  0.0808,  0.0060,  0.1290],\n",
      "          [ 0.1284,  0.0209,  0.2875,  0.0556,  0.1410],\n",
      "          [-0.2124, -0.3759, -0.1293, -0.0649, -0.1904],\n",
      "          [-0.2361, -0.0448, -0.0489, -0.1416, -0.0888]]]])\n",
      "b1 tensor([-0.8355, -7.0543, -1.0149,  2.1049])\n",
      "W2 tensor([[-0.1242,  0.1438],\n",
      "        [-0.2247, -0.0969],\n",
      "        [ 0.4435,  0.1973],\n",
      "        [ 0.5902, -0.1906]])\n",
      "b2 tensor([ 0.6196, -0.4773])\n",
      "=============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.72it/s, acc=0.5, loss=1.67] \n"
     ]
    }
   ],
   "source": [
    "train_custom_model(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4398f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:32:47.541080Z",
     "start_time": "2021-05-06T20:32:47.264Z"
    }
   },
   "outputs": [],
   "source": [
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
    "          [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
    "          [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
    "          [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
    "          [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
    "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
    "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
    "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
    "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
    "\n",
    "\n",
    "        [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
    "          [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
    "          [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
    "          [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
    "          [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
    "          [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
    "          [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
    "          [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
    "          [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]]], requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.1832, -0.0370,  0.1128,  0.0866], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.3232, -0.4252,  0.4799,  0.0261],\n",
    "        [ 0.3427,  0.1036,  0.1608,  0.3735]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4741, -0.3318], requires_grad=True))\n",
    "==============================================================\n",
    "\n",
    "100%|██████████| 3/3 [00:00<00:00,  3.54it/s, accuracy=1, loss=0.0179]\n",
    "\n",
    "Parameters\n",
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 0.0790, -0.1124, -0.4897,  0.0344, -0.2124],\n",
    "          [ 0.0958, -0.0652, -0.3758, -0.0051, -0.0255],\n",
    "          [ 0.5708,  0.5599,  0.1697,  0.4721,  0.4777],\n",
    "          [-0.0305, -0.0040, -0.4783,  0.1507,  0.0381],\n",
    "          [-0.0986, -0.1803, -0.4845, -0.1104, -0.0882]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0814,  0.1911, -0.1584, -0.1517,  0.1959],\n",
    "          [ 0.1277,  0.2616, -0.1562, -0.1174, -0.1183],\n",
    "          [-0.0246,  0.2475, -0.3215,  0.1575, -0.0211],\n",
    "          [ 0.2681, -0.0198, -0.1382,  0.0742, -0.0335],\n",
    "          [ 0.1747, -0.0624, -0.2083,  0.0141,  0.0499]]],\n",
    "\n",
    "\n",
    "        [[[-0.1807, -0.0838,  0.3246, -0.0201,  0.2108],\n",
    "          [ 0.1719,  0.0022,  0.0953,  0.1376,  0.0625],\n",
    "          [-0.3763, -0.1337,  0.0751, -0.0862, -0.3480],\n",
    "          [-0.0269,  0.0119,  0.2580, -0.1402, -0.1304],\n",
    "          [ 0.1204, -0.0184,  0.3138,  0.0230,  0.0772]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0314,  0.0601, -0.1361, -0.2162,  0.0881],\n",
    "          [-0.1532, -0.1635, -0.2369, -0.0513,  0.1170],\n",
    "          [ 0.1530,  0.1091,  0.1960,  0.2573,  0.3693],\n",
    "          [ 0.0578, -0.1672, -0.2089,  0.0355, -0.1465],\n",
    "          [-0.1075,  0.0372, -0.1994, -0.0976, -0.0731]]]], requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.2302, -0.1774,  0.0902,  0.1112], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.3022, -0.2247,  0.4435,  0.2730],\n",
    "        [ 0.3217, -0.0969,  0.1973,  0.1266]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4034, -0.2611], requires_grad=True))\n",
    "==============================================================\n",
    "Parameters\n",
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 6.7224e-02, -1.2410e-01, -5.0147e-01,  2.2663e-02, -2.2412e-01],\n",
    "          [ 8.4114e-02, -7.6974e-02, -3.8749e-01, -1.6871e-02, -3.4586e-02],\n",
    "          [ 6.1396e-01,  6.0695e-01,  2.1672e-01,  5.1914e-01,  5.1817e-01],\n",
    "          [-4.0968e-02, -1.5686e-02, -4.9003e-01,  1.3900e-01,  2.6409e-02],\n",
    "          [-1.1031e-01, -1.9200e-01, -4.9622e-01, -1.2210e-01, -9.9919e-02]]],\n",
    "\n",
    "\n",
    "        [[[ 8.1444e-02,  1.9109e-01, -1.5841e-01, -1.5174e-01,  1.9585e-01],\n",
    "          [ 1.2774e-01,  2.6158e-01, -1.5618e-01, -1.1738e-01, -1.1829e-01],\n",
    "          [-2.4599e-02,  2.4749e-01, -3.2151e-01,  1.5746e-01, -2.1074e-02],\n",
    "          [ 2.6811e-01, -1.9776e-02, -1.3821e-01,  7.4236e-02, -3.3496e-02],\n",
    "          [ 1.7467e-01, -6.2394e-02, -2.0830e-01,  1.4116e-02,  4.9941e-02]]],\n",
    "\n",
    "\n",
    "        [[[-1.9430e-01, -9.7377e-02,  3.7490e-01, -2.8138e-02,  1.9719e-01],\n",
    "          [ 1.5830e-01, -1.1385e-02,  1.4975e-01,  1.2540e-01,  4.8969e-02],\n",
    "          [-3.8983e-01, -1.4726e-01,  1.2952e-01, -9.9788e-02, -3.6157e-01],\n",
    "          [-4.0482e-02, -2.7019e-04,  3.1246e-01, -1.5380e-01, -1.4394e-01],\n",
    "          [ 1.0678e-01, -2.6421e-02,  3.6407e-01,  9.3816e-03,  6.3649e-02]]],\n",
    "\n",
    "\n",
    "        [[[ 3.4158e-02,  6.2900e-02, -1.3335e-01, -2.1350e-01,  9.0850e-02],\n",
    "          [-1.5044e-01, -1.6072e-01, -2.3417e-01, -4.8516e-02,  1.1910e-01],\n",
    "          [ 1.4289e-01,  9.8093e-02,  1.8493e-01,  2.4623e-01,  3.5975e-01],\n",
    "          [ 6.0202e-02, -1.6441e-01, -2.0615e-01,  3.8219e-02, -1.4373e-01],\n",
    "          [-1.0471e-01,  3.9919e-02, -1.9666e-01, -9.4895e-02, -7.0363e-02]]]],\n",
    "       requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.2531, -0.1774,  0.1167,  0.1058], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.5160, -0.2247,  0.7183,  0.1626],\n",
    "        [ 0.5355, -0.0969, -0.0775,  0.2370]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4744, -0.3320], requires_grad=True))\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591454a2-a616-49c7-9dd1-18d323492924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:26:41.748260Z",
     "start_time": "2021-05-11T09:26:41.305292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 1 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e570f1c4-26d0-4d56-9b8c-79d70773f7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:26:48.057633Z",
     "start_time": "2021-05-11T09:26:48.038929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задание 1 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной\n",
      "Суммарная ошибка = 1.9101\n"
     ]
    }
   ],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 1 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 1 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task1 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1543c",
   "metadata": {},
   "source": [
    "**Вывод.** Лоссы не сходятся, но в целом лосс у кастомной модели тоже уменьшается, так что непонятно..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e52a3a-fde2-401b-9517-b41492d445b9",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7295ab-2cf9-4eac-b525-62f3777bd4fa",
   "metadata": {},
   "source": [
    "Требуется реализовать классы пакетной нормализации **BatchNorm2d** и слоя выборки усреднением **AvgPool**. Реализовать класс модели, идентичной эталонной TorchGradientModel2\n",
    "\n",
    "Эталонная модель **TorchGradientModel2** состоит из:\n",
    "- Сверточный слой с 4 фильтрами размера $3\\times3$;\n",
    "- Пакетная нормализация;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Выборки усреднением с ашгом 2 и размером окна 2\n",
    "- Линейный слой с переменных количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb82edce-6429-4fd5-957a-d373e8ed46a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:57:31.444277Z",
     "start_time": "2021-05-11T06:57:31.426203Z"
    }
   },
   "outputs": [],
   "source": [
    "class TorchGradientModel2(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=0, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * int((input_size - 2) / 2) * int((input_size - 2) / 2), num_classes)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8dc616e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:03:56.478376Z",
     "start_time": "2021-05-11T07:03:56.441536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 4, 3, 3]             40\n",
      "├─BatchNorm2d: 1-2                       [-1, 4, 3, 3]             8\n",
      "├─ReLU: 1-3                              [-1, 4, 3, 3]             --\n",
      "├─AvgPool2d: 1-4                         [-1, 4, 1, 1]             --\n",
      "├─Flatten: 1-5                           [-1, 4]                   --\n",
      "├─Linear: 1-6                            [-1, 2]                   10\n",
      "==========================================================================================\n",
      "Total params: 58\n",
      "Trainable params: 58\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(TorchGradientModel2(), (1, 5, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0b11589-27fa-4201-ab3f-079ba27c8af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:05:23.817946Z",
     "start_time": "2021-05-11T07:05:23.390905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  4.83it/s, accuracy=1, loss=0.613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.0756,  0.1724, -0.1033],\n",
      "          [-0.2789,  0.0732,  0.2076],\n",
      "          [-0.3250,  0.0347,  0.0664]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0383, -0.0082, -0.1036],\n",
      "          [-0.2588,  0.0544, -0.2239],\n",
      "          [-0.2754, -0.1070, -0.2282]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1022,  0.2374,  0.2591],\n",
      "          [-0.2066,  0.1817,  0.2369],\n",
      "          [ 0.0037,  0.1406,  0.1886]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2550,  0.1784,  0.1423],\n",
      "          [-0.1265, -0.1858, -0.1581],\n",
      "          [ 0.2743, -0.0807,  0.0119]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.3872, -0.4048, -0.0715, -0.0355],\n",
      "        [-0.4527,  0.3023, -0.1591,  0.2246]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([0.2791, 0.4409], requires_grad=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 33.81it/s, accuracy=1, loss=0.0289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.0259,  0.2685,  0.0250],\n",
      "          [-0.5487, -0.1588,  0.0036],\n",
      "          [-0.2229,  0.1935,  0.2502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0895,  0.2329,  0.2510],\n",
      "          [-0.2041,  0.1931,  0.2436],\n",
      "          [ 0.0012,  0.1471,  0.1910]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2148,  0.1432,  0.1071],\n",
      "          [-0.1541, -0.2073, -0.1802],\n",
      "          [ 0.3285, -0.0178,  0.0750]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.1520, 0.8936, 1.0176, 1.0089], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.0831, -0.0773,  0.0192,  0.0311], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.5683, -0.2543,  0.1292, -0.0695],\n",
      "        [-0.6337,  0.1518, -0.3597,  0.2587]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([0.2583, 0.4617], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.0056,  0.4506,  0.2141],\n",
      "          [-0.5281,  0.0165,  0.1801],\n",
      "          [-0.5710, -0.0118,  0.0419]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0622,  0.2219,  0.2320],\n",
      "          [-0.1970,  0.2181,  0.2578],\n",
      "          [-0.0033,  0.1610,  0.1956]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1423,  0.0905,  0.0533],\n",
      "          [-0.1803, -0.2110, -0.1878],\n",
      "          [ 0.3841,  0.0628,  0.1539]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.1380, 0.8936, 1.0621, 1.0438], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.0380, -0.0773,  0.0665,  0.0752], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.5517, -0.2543,  0.2237, -0.1812],\n",
      "        [-0.6171,  0.1518, -0.4543,  0.3704]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([0.0864, 0.6335], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.0543,  0.4261,  0.1949],\n",
      "          [-0.5412,  0.0418,  0.2071],\n",
      "          [-0.5748,  0.0238,  0.0791]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0358,  0.2083,  0.2091],\n",
      "          [-0.1852,  0.2445,  0.2717],\n",
      "          [-0.0052,  0.1750,  0.1991]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0790,  0.0509,  0.0116],\n",
      "          [-0.1888, -0.1931, -0.1754],\n",
      "          [ 0.4069,  0.1133,  0.2021]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.2374, 0.8936, 1.1315, 1.0952], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.1598, -0.0773,  0.1372,  0.1136], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.6524, -0.2543,  0.3394, -0.2837],\n",
      "        [-0.7178,  0.1518, -0.5699,  0.4728]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([0.0516, 0.6684], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.0832,  0.4119,  0.1832],\n",
      "          [-0.5452,  0.0539,  0.2199],\n",
      "          [-0.5741,  0.0414,  0.0974]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1168,  0.2864,  0.2823],\n",
      "          [-0.1341,  0.2854,  0.3061],\n",
      "          [-0.1146,  0.0585,  0.0758]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0211,  0.0174, -0.0245],\n",
      "          [-0.1898, -0.1669, -0.1559],\n",
      "          [ 0.4160,  0.1505,  0.2371]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.2947, 0.8936, 1.1793, 1.1613], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.2254, -0.0773,  0.1196,  0.1541], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.7118, -0.2543,  0.3962, -0.3855],\n",
      "        [-0.7772,  0.1518, -0.6268,  0.5746]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0078,  0.7277], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.1081,  0.3985,  0.1720],\n",
      "          [-0.5477,  0.0643,  0.2308],\n",
      "          [-0.5723,  0.0567,  0.1133]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1277,  0.3379,  0.3355],\n",
      "          [-0.2641,  0.1927,  0.2128],\n",
      "          [-0.0752,  0.1422,  0.1572]]],\n",
      "\n",
      "\n",
      "        [[[-0.0039,  0.0138, -0.0889],\n",
      "          [-0.1654, -0.1191, -0.1747],\n",
      "          [ 0.4334,  0.1916,  0.2172]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.3490, 0.8936, 1.2035, 1.2353], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.2859, -0.0773,  0.1190,  0.1743], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.7682, -0.2543,  0.4240, -0.4782],\n",
      "        [-0.8336,  0.1518, -0.6546,  0.6674]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0497,  0.7696], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.1262,  0.3881,  0.1632],\n",
      "          [-0.5487,  0.0719,  0.2387],\n",
      "          [-0.5703,  0.0680,  0.1250]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1007,  0.3265,  0.3253],\n",
      "          [-0.2652,  0.2086,  0.2281],\n",
      "          [-0.0798,  0.1550,  0.1697]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133, -0.0366, -0.0798],\n",
      "          [-0.1491, -0.1442, -0.1389],\n",
      "          [ 0.4449,  0.1632,  0.2535]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.3923, 0.8936, 1.2301, 1.2874], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.3330, -0.0773,  0.1507,  0.1848], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.8131, -0.2543,  0.4572, -0.5359],\n",
      "        [-0.8785,  0.1518, -0.6878,  0.7251]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0680,  0.7880], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.1302,  0.3867,  0.1626],\n",
      "          [-0.5576,  0.0639,  0.2307],\n",
      "          [-0.5620,  0.0805,  0.1375]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0858,  0.3193,  0.3186],\n",
      "          [-0.2647,  0.2175,  0.2365],\n",
      "          [-0.0814,  0.1621,  0.1765]]],\n",
      "\n",
      "\n",
      "        [[[-0.0323, -0.0388, -0.0921],\n",
      "          [-0.1503, -0.1273, -0.1353],\n",
      "          [ 0.4405,  0.1775,  0.2578]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([-0.3109, -0.0148, -0.0940, -0.2151], requires_grad=True))\n",
      "('bn1.weight', Parameter containing:\n",
      "tensor([1.4215, 0.8936, 1.2478, 1.3328], requires_grad=True))\n",
      "('bn1.bias', Parameter containing:\n",
      "tensor([ 0.3544, -0.0773,  0.1706,  0.2055], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.8413, -0.2543,  0.4788, -0.5854],\n",
      "        [-0.9067,  0.1518, -0.7094,  0.7745]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0880,  0.8079], requires_grad=True))\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.1387,  0.3815,  0.1582],\n",
      "          [-0.5574,  0.0678,  0.2347],\n",
      "          [-0.5611,  0.0854,  0.1427]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2197, -0.0151,  0.0407],\n",
      "          [-0.2041, -0.0917, -0.1982],\n",
      "          [-0.2890, -0.3256, -0.2662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0750,  0.3136,  0.3133],\n",
      "          [-0.2639,  0.2240,  0.2426],\n",
      "          [-0.0823,  0.1672,  0.1815]]],\n",
      "\n",
      "\n",
      "        [[[-0.0455, -0.0402, -0.1006],\n",
      "          [-0.1507, -0.1149, -0.1327],\n",
      "          [ 0.4365,  0.1870,  "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 11650 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце \n",
    "# loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной \n",
    "# воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].detach().clone())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты \n",
    "# по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. \n",
    "# При желании можете поэкспериментировать с другими, которые реализовали \n",
    "# для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), \n",
    "                            lr=learning_rate, momentum=0, dampening=0, \n",
    "                            weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "    for param in torch_grad_model.named_parameters():\n",
    "        print(param)\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa3629",
   "metadata": {},
   "source": [
    "Найдём формулы для `backward()` при подаче на вход строчного вектора $\\vec{x}\\in R^{1\\times K}$. В [оригинальной статье](https://arxiv.org/pdf/1502.03167.pdf) на странице 4 есть все формулы для реализации backward pass. Сначала распишем, что мы вообще считаем в фазе тренировки:\n",
    "\n",
    "### Forward\n",
    "#### BatchNorm\n",
    "$$\n",
    "\\vec{\\widehat{x}} = \\frac{\\vec{x} - \\widehat{\\mu}}{\\sqrt{\\widehat{\\sigma}^2 + \\varepsilon}},\\\\\n",
    "\\vec{\\widehat{\\mu}} = \\frac{1}{N}\\sum\\limits_{i=1}^N \\vec{x}_i,\\ \\widehat{\\sigma} = \\frac{1}{N}\\sum\\limits_{i=1}^N \\big(\\vec{x} - \\vec{\\widehat{\\mu}}\\big)^2.\\\\\n",
    "$$\n",
    "#### Scaling\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K},\n",
    "\\gamma \\in \\mathbb{R}^{1 \\times K},\n",
    "\\beta \\in \\mathbb{R}^{1 \\times K}, \\\\\n",
    "\\text{Scaling}(x) = \\gamma x + \\beta,\\ \n",
    "\\text{Scaling}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "где $\\gamma$ и $\\beta$ &mdash; обучаемые параметры слоя.\n",
    "\n",
    "### Backward\n",
    "Теперь уже займёмся градиентами:\n",
    "#### BatchNorm\n",
    "$$\n",
    "\\frac{\\partial \\widehat{x}_i}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma^2 + \\varepsilon}},\n",
    "\\\\\n",
    "\\frac{\\partial \\mu}{\\partial x_i} = \\frac{1}{N},\n",
    "\\\\\n",
    "\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2(x_i - \\mu)}{N},\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\sum\\limits_{i=1}^N \\frac{\\partial L}{\\partial \\widehat{x}_i} \\frac{-1}{\\sqrt{\\sigma^2 + \\varepsilon}}, \n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial \\sigma^2} = \\sum_{i=1}^N\\frac{\\partial L}{\\partial \\widehat{x}_i}\\cdot\\bigg(-0.5(x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-3/2}\\bigg),\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial x_j} = \\frac{\\partial L}{\\partial \\widehat{x}_j} \\cdot \\frac{1}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{N} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{2(x_i - \\mu)}{N}.\n",
    "% \\\\= \\frac{1}{N \\sqrt{\\sigma^2 + \\varepsilon}} \\bigg(N \\frac{\\partial L}{\\partial \\widehat{x}_j} - \\sum\\limits_{i=1}^N \\frac{\\partial L}{\\partial \\widehat{x}_j} - \\widehat{x}_i \\sum\\limits_{i=1}^N \\frac{\\partial L}{\\partial \\widehat{x}_j}\\cdot \\widehat{x}_j\\bigg).\n",
    "$$\n",
    "#### Scaling\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\widehat{x}_i} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial y_i} \\gamma, \\\\\n",
    "\\frac{\\partial L}{\\partial \\gamma} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial y_i} \\widehat{x}_i, \\\\\n",
    "\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial y_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1114bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Task2Modules.OtherLayers import BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d9137",
   "metadata": {},
   "source": [
    "Теперь разберёмся с average pooling. Если расписать на листочке для простых примеров формулы, то становится понятно, что `backward()` очень похож на `backward()` в свёрточном слое, только теперь нет параметров (то есть не нужно считать градиент по параметрам), все значения фильтра одинаковые (то есть вращать ничего не надо), паддинга нет (то есть не надо делать дополнительные расширения границ). Таким образом, из преобразований остаётся только раздвижение строк и столбцов друг от друга на `self.s - 1` с помощью функции `delate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8983315a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:20:09.652500Z",
     "start_time": "2021-05-11T07:20:09.638285Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AvgPool' from 'Task2Modules.CNN' (/home/topcoder2k/CodeProjects/PythonProjects/scidev-TopCoder2K/Task2Modules/CNN.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c59f480ab90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTask2Modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAvgPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AvgPool' from 'Task2Modules.CNN' (/home/topcoder2k/CodeProjects/PythonProjects/scidev-TopCoder2K/Task2Modules/CNN.py)"
     ]
    }
   ],
   "source": [
    "from Task2Modules.CNN import AvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77309d-ad2e-46e8-9ffd-4e157006ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Task2Modules.CustomModels import CustomModel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651e71c-2a12-45c6-8f05-32c23fca32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model2(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel2(input_size = X.shape[-1], num_classes=num_classes)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr = learning_rate, params = model.get_params())      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0 \n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item() / X.shape[0]\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7053a57-ae3b-4f05-a73a-7d8b28079479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_model2(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d4eb9-b1ed-4919-8d37-98f12cbae3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2684-a0a0-4025-8713-01e696c4827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 2 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 2 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cb689-abb1-458f-b851-046583c29220",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71114630-0f89-4b70-8c7c-cbe5091f0440",
   "metadata": {},
   "source": [
    "Обучить ранее реализованную сеть **CustomModel2** на реальных данных из выборки digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96e6f0-3d16-4656-aaf9-299c5cd423a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aa79a-730a-4b53-9a6d-7769e5ab164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_numbers():\n",
    "    # Загружаем выборку digits\n",
    "    digits = load_digits(n_class=10, return_X_y=False, as_frame=False)\n",
    "\n",
    "    # ПРОВЕСТИ НЕОБХОДИМЫЕ ПРЕОБРАЗОВАНИЯ\n",
    "    \n",
    "    return train_x, train_label, val_x, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e861b8-49e4-4800-872a-c8ac7ee4093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_label, val_x, val_label = get_data_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766a8b7-dc86-4572-baa0-ecf80dec5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "num_classes = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2(num_classes, input_size=8)\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. При желании можете поэкспериментировать с другими, которые реализовали для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(train_x[0:10]) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, train_label[0:10])\n",
    "    loss_history.loc[e, 'loss_train_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == train_label[0:10]).item()\n",
    "\n",
    "    train_acc /= train_x[0:10].shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123a514-c557-44f2-baed-77f6b4adf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, learning_rate, loss_history, train_x, train_label, val_x, val_label):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e12010-8508-4c62-bf6d-e26cb66c62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "train_model(epochs, learning_rate, loss_history, train_x[0:10], train_label[0:10], val_x[0:10], val_label[0:10])\n",
    "\n",
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603aeed-717d-4d6e-ad07-a652a284e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute(loss_history[['loss_train_custom']].to_numpy()-loss_history[['loss_train_pt']].to_numpy()).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 3 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 3 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a740226-459e-46c6-92ab-c38dac6c3468",
   "metadata": {},
   "source": [
    "### Необязательное задание\n",
    "Создать свою архитектуру свёрточной сети и обучиться на полной выборке digits, получив высокое качество классификации. Сравнить скорость и финальное качество обучения при применении разных алгоритмов обучения (SGD с моментом, Adam), регуляризации весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac56e4-981f-46b3-9ffd-54e63054f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
