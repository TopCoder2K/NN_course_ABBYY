{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33a7d46-d93f-40b0-8256-97a1c43344e6",
   "metadata": {},
   "source": [
    "## Домашнее задание по свёрточным сетям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbc722-2c7e-41a1-b25f-913c32228a2d",
   "metadata": {},
   "source": [
    "Сутью домашнего задания является последовательная реализация базовых операций, применяемых в свёрточных сетях с использованием операций над тензорами PyTorch, но без применения модулей torch.nn. Студенты должны самостоятельно реализовать как прямой и обратный проходы слоёв, так и классы нейронных сетей.\n",
    "\n",
    "Правильность выполнения задания будет проверяться идентичностью прохождения процесса обучения в тех же архитектурах, выполненных с применением модулей и алгоритмов PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af498e6-0c14-4731-a899-ad5f6b19766b",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78d40c-6d89-49d5-be7f-43f9e8607921",
   "metadata": {},
   "source": [
    "В первом задании требуется реализовать классы двухмерной свёртки **Conv**, линейного слоя **Fc**, алгоритм обучения нейронной сети **SGD**, функцию активации **ReLU**, **Softmax** и функцию эмпирического риска **CrossEntropyLoss**.\n",
    "\n",
    "Свёрточные и полносвязные слои должны реализовывать операцию сдвига (*bias*, *b*). \n",
    "Сверить формулы прямого прохода можно в документации по [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) и [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Далее, следует реализовать класс модели, который включает в себя реализованные выше компоненты. Требуется повторить параметры обучения и архитектуру, реализованную ниже с помощью torch.nn модулей. Критерием правильности решения будет совпадение значений эмпирического риска при обучении обеих реализаций сетей на одних и тех же данных, с теми же параметрами и с одинаковыми начальными инициализациями весов.\n",
    "\n",
    "Данные для обучения состоят из 4 примеров вертикальных и горизонтальных линий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a74f3a-578c-4ca9-aa1c-1834e19b7bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:11.457577Z",
     "start_time": "2021-05-06T19:29:03.852579Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "\n",
    "_ = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a852825-7062-4829-a9e3-3e341cc46076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:11.468307Z",
     "start_time": "2021-05-06T19:29:11.459323Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y, num_classes):\n",
    "    N = y.shape[0]\n",
    "    Z = torch.zeros((N, num_classes))\n",
    "    Z[torch.arange(N), y] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b94731e-4897-4ad1-96a5-b128216b6922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:12.416041Z",
     "start_time": "2021-05-06T19:29:11.475328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape [Batch, Channels, Height, Width] = torch.Size([4, 1, 5, 5])\n",
      "Labels shape = torch.Size([4])\n",
      "Mean and standard deviation before normalization = 49.90, 97.45\n",
      "Mean and standard deviation after normalization = 0.00, 1.00\n",
      "Train batch\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJAUlEQVR4nO3dz2ucBR7H8c/HNP4AFzwkB2nKxoPIlsIqDEXoRYqH+gO9KuhJ6GWFCoLo0X9AvHgpKi4oiqAHERcpqIjgr1Gr2I1CKV0sCk0QsSIotR8PMyxdN+k8M5lnnszX9wsCmU545kPJO0/yJMw4iQDUcVnXAwBMF1EDxRA1UAxRA8UQNVDMrjYOurS0lNXV1TYODUDS6dOntbGx4c3uayXq1dVV9fv9Ng4NQFKv19vyPr79BoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGimkUte1Dtr+2fdL2o22PAjC5kVHbXpD0lKTbJO2VdK/tvW0PAzCZJmfq/ZJOJjmV5FdJL0m6u91ZACbVJOrdkr656PaZ4b/9D9uHbfdt99fX16e1D8CYmkS92dOQ/t+r6iU5mqSXpLe8vLz9ZQAm0iTqM5L2XHR7RdK37cwBsF1Nov5Y0vW2r7N9uaR7JL3W7iwAkxr5ZP5Jztt+UNKbkhYkPZvkROvLAEyk0St0JHlD0hstbwEwBfxFGVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxTR6kgTgz+Dnn3/uekJjFy5c2PI+ztRAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxI6O2/azts7a/nMUgANvT5Ez9nKRDLe8AMCUjo07yrqTvZ7AFwBTwMzVQzNSitn3Ydt92f319fVqHBTCmqUWd5GiSXpLe8vLytA4LYEx8+w0U0+RXWi9Kel/SDbbP2H6g/VkAJjXyFTqS3DuLIQCmg2+/gWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooxkmmftArrrgiKysrUz9uG06dOtX1BOwQt9xyS9cTGuv3+zp37pw3u48zNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8WMjNr2Httv216zfcL2kVkMAzCZXQ0+5rykh5N8avsvkj6xfSzJv1veBmACI8/USb5L8unw/XOS1iTtbnsYgMk0OVP/l+1VSTdJ+nCT+w5LOixJu3aNdVgAU9T4QpntqyW9IumhJD/+8f4kR5P0kvQuu4zrb0BXGtVne1GDoF9I8mq7kwBsR5Or35b0jKS1JE+0PwnAdjQ5Ux+QdL+kg7aPD99ub3kXgAmNvKKV5D1Jm768B4CdhytaQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U08rTfu7bt08ffPBBG4eeusXFxa4nAGPr9Xpb3seZGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKGZk1LavtP2R7c9tn7D9+CyGAZhMk6cz+kXSwSQ/2V6U9J7tfyWZj+crAv5kRkadJJJ+Gt5cHL6lzVEAJtfoZ2rbC7aPSzor6ViSD1tdBWBijaJO8luSGyWtSNpve98fP8b2Ydt92/2NjY0pzwTQ1FhXv5P8IOkdSYc2ue9okl6S3tLS0nTWARhbk6vfy7avGb5/laRbJX3V8i4AE2py9ftaSf+0vaDBF4GXk7ze7iwAk2py9fsLSTfNYAuAKeAvyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKbJM5+MzbYWFxfbODSAEThTA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UEzjqG0v2P7M9uttDgKwPeOcqY9IWmtrCIDpaBS17RVJd0h6ut05ALar6Zn6SUmPSLqw1QfYPmy7b7u/vr4+jW0AJjAyatt3Sjqb5JNLfVySo0l6SXrLy8tTGwhgPE3O1Ack3WX7tKSXJB20/XyrqwBMbGTUSR5LspJkVdI9kt5Kcl/rywBMhN9TA8WM9bI7Sd6R9E4rSwBMBWdqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKcZLpH9Rel/SfKR92SdLGlI/ZpnnaO09bpfna29bWvybZ9Bk+W4m6Dbb7SXpd72hqnvbO01ZpvvZ2sZVvv4FiiBooZp6iPtr1gDHN09552irN196Zb52bn6kBNDNPZ2oADRA1UMxcRG37kO2vbZ+0/WjXey7F9rO2z9r+susto9jeY/tt22u2T9g+0vWmrdi+0vZHtj8fbn28601N2F6w/Znt12f1mDs+atsLkp6SdJukvZLutb2321WX9JykQ12PaOi8pIeT/E3SzZL+sYP/b3+RdDDJ3yXdKOmQ7Zu7ndTIEUlrs3zAHR+1pP2STiY5leRXDV558+6ON20pybuSvu96RxNJvkvy6fD9cxp88u3udtXmMvDT8Obi8G1HX+W1vSLpDklPz/Jx5yHq3ZK+uej2Ge3QT7x5ZntV0k2SPux4ypaG38oel3RW0rEkO3br0JOSHpF0YZYPOg9Re5N/29FfoeeN7aslvSLpoSQ/dr1nK0l+S3KjpBVJ+23v63jSlmzfKelskk9m/djzEPUZSXsuur0i6duOtpRje1GDoF9I8mrXe5pI8oMGr766k69dHJB0l+3TGvzIeND287N44HmI+mNJ19u+zvblGrzw/WsdbyrBtiU9I2ktyRNd77kU28u2rxm+f5WkWyV91emoS0jyWJKVJKsafM6+leS+WTz2jo86yXlJD0p6U4MLOS8nOdHtqq3ZflHS+5JusH3G9gNdb7qEA5Lu1+Ascnz4dnvXo7ZwraS3bX+hwRf6Y0lm9muiecKfiQLF7PgzNYDxEDVQDFEDxRA1UAxRA8UQNVAMUQPF/A5c2OfqTvoRGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Обучение будет вестись на следующих данных\n",
    "def get_data():\n",
    "    # Задача классификации вертикальных и горизонтальных линий\n",
    "    # Данные\n",
    "    vert1 = [[0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0]]\n",
    "\n",
    "    vert2 = [[0, 0, 220, 40, 0],\n",
    "             [0, 0, 250, 10, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 10, 250, 0, 0],\n",
    "             [0, 40, 220, 0, 0]]\n",
    "\n",
    "    hor1 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [250, 250, 250, 250, 250],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    hor2 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 20],\n",
    "            [220, 250, 250, 250, 200],\n",
    "            [10, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    data = [vert1, vert2, hor1, hor2]\n",
    "    labels = [0, 0, 1, 1]\n",
    "    \n",
    "    train_x = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return train_x, labels\n",
    "\n",
    "train_x, labels = get_data()\n",
    "print(f'Train data shape [Batch, Channels, Height, Width] = {train_x.shape}')\n",
    "print(f'Labels shape = {labels.shape}')\n",
    "\n",
    "\n",
    "train_mean = torch.mean(train_x)\n",
    "train_std = torch.std(train_x)\n",
    "batch = (train_x - train_mean) / train_std\n",
    "\n",
    "print(f'Mean and standard deviation before normalization = {train_mean.item():.2f}, {train_std.item():.2f}')\n",
    "print(f'Mean and standard deviation after normalization = {torch.mean(batch).item():.2f}, {torch.std(batch).item():.2f}')\n",
    "\n",
    "print('Train batch')\n",
    "for img in batch:\n",
    "    plt.imshow(img.squeeze(0), cmap='Greys')  # Цвета инвертированы. Чем темнее, тем значение пикселя больше\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2d43-d5d9-46cb-92f4-1b55e8870dbe",
   "metadata": {},
   "source": [
    "Реализуем эталонную модель **TorchGradientModel**, состоящую из следующих модулей:\n",
    "- Сверточный слой с 4 фильтрами размера $5\\times5$;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Линейный слой с 2 выходными нейронами для классов горизонтальной и вертикальной линий\n",
    "\n",
    "Вопрос:\n",
    "Каким ещё образом можно осуществить бинарную классификацию, не используя линейный слой, с 2 выходными нейронами?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f54ef4-1d73-4e63-ba2b-f904df6534cf",
   "metadata": {},
   "source": [
    "**Ответ.** Можно подобрать размер свёрток так, чтобы на выходе получалось два канала, в каждом из которых будет тензор вероятности принадлежности к классам (в нашем случае детекция не нужна, поэтому достаточно на выходе получить 2 тензора размера 1 на 1, то есть output.shape = 2x1x1).\n",
    "\n",
    "А ещё можно попробовать с одним нейроном: применять какую-нибудь сигмоиду и смотреть, если значение > 0.5, то отправляем в класс 1, иначе --- в класс 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da78879f-cbf9-41d5-98d3-806367c5b86a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:12.426047Z",
     "start_time": "2021-05-06T19:29:12.418749Z"
    }
   },
   "outputs": [],
   "source": [
    "class TorchGradientModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, padding=0, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * 1 * 1, 2)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c48edb-7561-46c0-a045-f739adb76b97",
   "metadata": {},
   "source": [
    "Дополнительная информация может быть найдена в комментариях к коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6dedb7-e1ce-4283-bd3c-650e3d5aca03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:12.745474Z",
     "start_time": "2021-05-06T19:29:12.427535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Данная сеть обучается за 3 эпохи, что удобно для процесса отладки.\n",
    "learning_rate = 1\n",
    "epochs = 3\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "# torch_model_params[0] - тензор с весами фильтров W свёрточного слоя\n",
    "# torch_model_params[1] - тензор с весами сдвигов b свёрточного слоя\n",
    "# torch_model_params[2] - тензор с весами W линейного слоя\n",
    "# torch_model_params[3] - тензор с весами сдвигов b линейного слоя\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета \n",
    "# (для I-го слоя - это кол-во изображений),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для данного эксперимента используется самый простой алгоритм обучения без моментов.\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, \n",
    "                            momentum=0, dampening=0, weight_decay=0, \n",
    "                            nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1425e7-8003-42e7-bac1-7f13d7852a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:29:13.933030Z",
     "start_time": "2021-05-06T19:29:12.756165Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.88it/s, accuracy=1, loss=0.0179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
      "          [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
      "          [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
      "          [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
      "          [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
      "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
      "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
      "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
      "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
      "\n",
      "\n",
      "        [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
      "          [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
      "          [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
      "          [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
      "          [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
      "          [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
      "          [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
      "          [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
      "          [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.1832, -0.0370,  0.1128,  0.0866], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.3232, -0.4252,  0.4799,  0.0261],\n",
      "        [ 0.3427,  0.1036,  0.1608,  0.3735]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4741, -0.3318], requires_grad=True))\n",
      "==============================================================\n",
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.0790, -0.1124, -0.4897,  0.0344, -0.2124],\n",
      "          [ 0.0958, -0.0652, -0.3758, -0.0051, -0.0255],\n",
      "          [ 0.5708,  0.5599,  0.1697,  0.4721,  0.4777],\n",
      "          [-0.0305, -0.0040, -0.4783,  0.1507,  0.0381],\n",
      "          [-0.0986, -0.1803, -0.4845, -0.1104, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0814,  0.1911, -0.1584, -0.1517,  0.1959],\n",
      "          [ 0.1277,  0.2616, -0.1562, -0.1174, -0.1183],\n",
      "          [-0.0246,  0.2475, -0.3215,  0.1575, -0.0211],\n",
      "          [ 0.2681, -0.0198, -0.1382,  0.0742, -0.0335],\n",
      "          [ 0.1747, -0.0624, -0.2083,  0.0141,  0.0499]]],\n",
      "\n",
      "\n",
      "        [[[-0.1807, -0.0838,  0.3246, -0.0201,  0.2108],\n",
      "          [ 0.1719,  0.0022,  0.0953,  0.1376,  0.0625],\n",
      "          [-0.3763, -0.1337,  0.0751, -0.0862, -0.3480],\n",
      "          [-0.0269,  0.0119,  0.2580, -0.1402, -0.1304],\n",
      "          [ 0.1204, -0.0184,  0.3138,  0.0230,  0.0772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0601, -0.1361, -0.2162,  0.0881],\n",
      "          [-0.1532, -0.1635, -0.2369, -0.0513,  0.1170],\n",
      "          [ 0.1530,  0.1091,  0.1960,  0.2573,  0.3693],\n",
      "          [ 0.0578, -0.1672, -0.2089,  0.0355, -0.1465],\n",
      "          [-0.1075,  0.0372, -0.1994, -0.0976, -0.0731]]]], requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.2302, -0.1774,  0.0902,  0.1112], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.3022, -0.2247,  0.4435,  0.2730],\n",
      "        [ 0.3217, -0.0969,  0.1973,  0.1266]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4034, -0.2611], requires_grad=True))\n",
      "==============================================================\n",
      "Parameters\n",
      "('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 6.7224e-02, -1.2410e-01, -5.0147e-01,  2.2663e-02, -2.2412e-01],\n",
      "          [ 8.4114e-02, -7.6974e-02, -3.8749e-01, -1.6871e-02, -3.4586e-02],\n",
      "          [ 6.1396e-01,  6.0695e-01,  2.1672e-01,  5.1914e-01,  5.1817e-01],\n",
      "          [-4.0968e-02, -1.5686e-02, -4.9003e-01,  1.3900e-01,  2.6409e-02],\n",
      "          [-1.1031e-01, -1.9200e-01, -4.9622e-01, -1.2210e-01, -9.9919e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1444e-02,  1.9109e-01, -1.5841e-01, -1.5174e-01,  1.9585e-01],\n",
      "          [ 1.2774e-01,  2.6158e-01, -1.5618e-01, -1.1738e-01, -1.1829e-01],\n",
      "          [-2.4599e-02,  2.4749e-01, -3.2151e-01,  1.5746e-01, -2.1074e-02],\n",
      "          [ 2.6811e-01, -1.9776e-02, -1.3821e-01,  7.4236e-02, -3.3496e-02],\n",
      "          [ 1.7467e-01, -6.2394e-02, -2.0830e-01,  1.4116e-02,  4.9941e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9430e-01, -9.7377e-02,  3.7490e-01, -2.8138e-02,  1.9719e-01],\n",
      "          [ 1.5830e-01, -1.1385e-02,  1.4975e-01,  1.2540e-01,  4.8969e-02],\n",
      "          [-3.8983e-01, -1.4726e-01,  1.2952e-01, -9.9788e-02, -3.6157e-01],\n",
      "          [-4.0482e-02, -2.7019e-04,  3.1246e-01, -1.5380e-01, -1.4394e-01],\n",
      "          [ 1.0678e-01, -2.6421e-02,  3.6407e-01,  9.3816e-03,  6.3649e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4158e-02,  6.2900e-02, -1.3335e-01, -2.1350e-01,  9.0850e-02],\n",
      "          [-1.5044e-01, -1.6072e-01, -2.3417e-01, -4.8516e-02,  1.1910e-01],\n",
      "          [ 1.4289e-01,  9.8093e-02,  1.8493e-01,  2.4623e-01,  3.5975e-01],\n",
      "          [ 6.0202e-02, -1.6441e-01, -2.0615e-01,  3.8219e-02, -1.4373e-01],\n",
      "          [-1.0471e-01,  3.9919e-02, -1.9666e-01, -9.4895e-02, -7.0363e-02]]]],\n",
      "       requires_grad=True))\n",
      "('conv1.bias', Parameter containing:\n",
      "tensor([ 0.2531, -0.1774,  0.1167,  0.1058], requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.5160, -0.2247,  0.7183,  0.1626],\n",
      "        [ 0.5355, -0.0969, -0.0775,  0.2370]], requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([ 0.4744, -0.3320], requires_grad=True))\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "    # Можете выводить веса сети для прямого сравнения со своей реализацей\n",
    "    print('Parameters')\n",
    "    for param in torch_grad_model.named_parameters():\n",
    "        print(param)\n",
    "    print('==============================================================')\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты также можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\"\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605478d-3157-4556-a5af-3fae08b02da1",
   "metadata": {},
   "source": [
    "Ниже представлены заготовки (шаблоны) классов, колторые требуется реализовать. **Conv**, **Fc** и **ReLU** должны иметь методы **forward** и **backward** для прямого и обратного прохода по сети. При прямом проходе следует кэшировать данные, которые потребуются для вычисления градиента.\n",
    "\n",
    "**Свёртки**\n",
    "\n",
    "При прямом проходе нужно брать фильтры поочерёдно и проводить свёртку со входным тензором. Каждая операция свёртки даёт 2-мерную матрицу на выходе. Для получения итоговой карты признаков следует сконкатенировать эти матрицы, чтобы получить тензор рамерами [Размер батча, Количество каналов, Высота, Ширина]. Реализовывать можно как с помощью вложенных циклов, так и с применением векторизации. В данном задании важно не время работы, но точность вычислений.\n",
    "\n",
    "В первом модуле заданий уже была показана реализация обратного прохода по линейному слою для подсчёта частных производных по эмпирическому риску, которые использовались для обновления весов слоя. Расчёт частных производных в свёрточных слоях идеологически тот же. Требуется посчитать частные производные по dX предыдущему входу слоя, dW по весам фильтров и db по сдвигам. Пусть dZ – это градиент ошибки к выходу текущего свёрточного слоя (передаётся от предыдущего слоя при обратном проходе сети), тогда\n",
    "\n",
    "$dX += \\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W} W_c\\times dZ_{hw}$,\n",
    "\n",
    "где $W_c$ – это фильтр, а $dZ_{hw}$ – скаляр, соответствующий градиенту эмпирического риска к выходу текущего свёрточного слоя $Z$ в $n$ строке и $w$ столбце. Так как при прямом проходе фильтр $W_c$ влияет на все значения канала с карты признаков, то мы умножаем один и тот же фильтр $W_c$ с разными $dZ$ в пределах канала $с$, суммируя результаты.\n",
    "В numpy эта операция выглядела бы так\n",
    "\n",
    " dX[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    " \n",
    "В PyTorch очерёдность каналов иная: [Batch, Channel, Height, Width].\n",
    "\n",
    "Производная одного фильтра относительно эмпирического риска  считается по формуле\n",
    "\n",
    "$dW_c+=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}x_{slice} \\times dZ_{hw}$,\n",
    "\n",
    "где $x_{slice}$ относится к отрезку из входного тензора, который был использован в прямом проходе, чтобы получить активацию $Z_{ij}$. Таким образом мы получим градиент фильтра $W$ относительно данного отрезка. Так как в рамках свёрточного слоя для разных отрезков мы использовали тот же фильтр $W$, то мы складываем эти градиенты, чтобы получить $dW$.\n",
    "\n",
    "В numpy подобная операция реализуется так:\n",
    "\n",
    "dW[:,:,:,c] += x_slice * dZ[i, h, w, c]\n",
    "\n",
    "Производная по сдвигам считается как сумма всех градиентов выхода свёрточного слоя:\n",
    "\n",
    "$db=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}dZ_{hw}$\n",
    "\n",
    "В numpy реализовывалась бы так:\n",
    "\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "\n",
    "\n",
    "Для реализации выполнения обратного прохода градиента идентичным nn.CrossEntropyLoss(reduction='mean') образом, значения $dW$ и $db$ следует делить на размер пакета (batch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73eb63",
   "metadata": {},
   "source": [
    "**Замечание.** Хочется понять откуда взялись эти формулы... Вот [тут](https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509) и [тут](https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c) выводят с примерами, но без учёта `stride` и `padding`.\n",
    "\n",
    "Я сам пытался вывести часа 3 формулы в общем случае, но c градиентом по входу потрепел фиаско. Как же хорошо, что мне скинули в итоге вот эту [статью](https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "327ef530-74d6-4ab3-b935-d5766410aee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:29:51.458306Z",
     "start_time": "2021-05-06T20:29:51.430102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Наивная реализация свёртки, медленная\n",
    "class Conv():  # TODO: в чём смысл ставить круглые скобки???\n",
    "    def __init__(self, nb_filters: int, filter_size: int, nb_channels: int, \n",
    "                 stride: int = 1, padding: int = 0, \n",
    "                 sanity_check: bool = True):\n",
    "        self.num_filters = nb_filters\n",
    "        self.f = filter_size\n",
    "        self.n_C = nb_channels\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "#         self.sanity_check = sanity_check  # использовать не обязательно\n",
    "\n",
    "        self.cache = None  # Для хранения данных прямого прохода сети, \n",
    "                           # которые потребуются при обратном проходе\n",
    "        \n",
    "        self.W = torch_model_params[0]  # (n_C, n_C_prev, filter_size, filter_size)\n",
    "        self.dW = torch.zeros(self.W.shape)\n",
    "        \n",
    "        self.b = torch_model_params[1]  # (n_C)\n",
    "        self.db =  torch.zeros(self.b.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def single_conv(x_slice, W, b):\n",
    "        \"\"\"\n",
    "        Выполняет скалярное произведение двух матриц. \n",
    "        Нужна для операции свёртки.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        x_slice : torch.tensor, shape = (n_C_prev, filter_size, filter_size)\n",
    "            Часть входных данных, над которыми сейчас идёт свёртка\n",
    "        W : torch.tensor, shape = (n_C_prev, filter_size, filter_size)\n",
    "            Параметры свёртки\n",
    "        b : torch.tensor, shape = (1, 1, 1)\n",
    "            Параметр смещения для данного фильтра\n",
    "        \n",
    "        Возвращает\n",
    "        ----------\n",
    "        float\n",
    "            Результат применения операции свёртки.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Поэлементное произведение\n",
    "        s = torch.mul(x_slice, W)\n",
    "        # Сумма произведений\n",
    "        g = torch.sum(s)\n",
    "        # Сдвиг\n",
    "        g = g + b\n",
    "        return g\n",
    "    \n",
    "    def _dilate(self, X):\n",
    "        \"\"\"\n",
    "        Вставляет в каждом фильтре после каждой строки/столбца \n",
    "        stride-1 нулевых строк/столбцов. Нужна для backpropagation.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        X : torch.tensor, shape = (n_C, H, W)\n",
    "            Тензор для преобразования.\n",
    "        \n",
    "        Возвращает\n",
    "        ----------\n",
    "        torch.tensor, shape = (n_C, new_H, new_W)\n",
    "            Преобразованный тензор.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_C, H, W = X.shape\n",
    "        \n",
    "        new_H, new_W = H + (H - 1) * (self.s - 1), W + (W - 1) * (self.s - 1)\n",
    "        X_dilated = torch.zeros((n_C, new_H, new_W))\n",
    "        \n",
    "#         for c in range(n_C):\n",
    "        for i in range(0, new_H, self.s):\n",
    "            for j in range(0, new_W, self.s):\n",
    "                X_dilated[:, i, j] = X[:, i // self.s, j // self.s]\n",
    "                \n",
    "        return X_dilated\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotate180(X):\n",
    "        \"\"\"\n",
    "        Поворачивает тензор на 180 градусов с помощью двух симметричных\n",
    "        отображений.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        X : torch.tensor, shape = (H, W)\n",
    "            Тензор для преобразования.\n",
    "        \n",
    "        Возвращает\n",
    "        ----------\n",
    "        torch.tensor\n",
    "            Повёрнутый на 180 градусов тензор.\n",
    "        \"\"\"\n",
    "        \n",
    "        H, W = X.shape\n",
    "        \n",
    "        X_rotated = torch.zeros(X.shape)\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                X_rotated[i, j] = X[H - 1 - i, W - 1 - j]\n",
    "                \n",
    "        return X_rotated\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямой проход для свёрточного слоя.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        X : torch.tensor, shape = (m, n_C_prev, n_H_prev, n_W_prev).\n",
    "            Выход предыдущего свёрточного слоя.\n",
    "        \n",
    "        Возвращает\n",
    "        ----------\n",
    "        torch.tensor, shape = (m, n_C, n_H, n_W)\n",
    "        \"\"\"\n",
    "        \n",
    "        m, n_C_prev, n_H_prev, n_W_prev = X.shape\n",
    "        \n",
    "        # По формулам из доков Conv2d, dilation = 0\n",
    "        n_H = int((n_H_prev + 2 * self.p - self.f) / self.s) + 1\n",
    "        n_W = int((n_W_prev + 2 * self.p - self.f) / self.s) + 1\n",
    "        \n",
    "        Z = torch.zeros((m, self.n_C, n_H, n_W))\n",
    "        # Так как ничего не указано, заполняем нулями\n",
    "        X_padded = torch.nn.functional.pad(\n",
    "            X, (self.p, self.p, self.p, self.p), mode='constant', value=0\n",
    "        )\n",
    "        \n",
    "        for i in range(m):\n",
    "            x_padded = X_padded[i]\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    # Вычислим положение текущего окна\n",
    "                    top_left_h = h * self.s\n",
    "                    top_left_w = w * self.s\n",
    "                    bottom_right_h = top_left_h + self.f\n",
    "                    bottom_right_w = top_left_w + self.f\n",
    "                    \n",
    "                    # Текущее окно\n",
    "                    window = x_padded[:, top_left_h:bottom_right_h, \n",
    "                                      top_left_w:bottom_right_w]\n",
    "                    for c in range(self.n_C):                        \n",
    "                        # Применим свёртку\n",
    "                        Z[i, c, h, w] = Conv.single_conv(window, self.W[c], \n",
    "                                                         self.b[c])\n",
    "\n",
    "        # Сохраняем то, что пригодится для обратного прохода\n",
    "        self.cache = {'input' : X, 'output' : Z}\n",
    "                        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Распространяет градиент ошибки от предыдущего слоя в текущий \n",
    "        свёрточный слой.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        dZ : torch.tensor, shape = (m, n_C, n_H, n_W)\n",
    "            Градиент, пришедший от следующего слоя.\n",
    "            \n",
    "        Возвращает\n",
    "        ----------\n",
    "        dX : torch.tensor, shape = (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "            Ошибка текущего свёрточного слоя.\n",
    "        self.dW : torch.tensor, shape = (n_C, n_C_prev, filter_size, filter_size)\n",
    "            Градиент по весам.\n",
    "        self.db : torch.tensor, shape = (n_C)\n",
    "            Градиент по сдвигам.\n",
    "        \"\"\"\n",
    "        \n",
    "        m, n_C_prev, n_H_prev, n_W_prev = self.cache['input'].shape\n",
    "        m, n_C, n_H, n_W = self.cache['output'].shape\n",
    "        \n",
    "        # Форма пришедшего градиента должна совпадать с формой выхода слоя\n",
    "        assert dZ.shape == self.cache['output'].shape\n",
    "\n",
    "        dX = np.zeros(\n",
    "            (m, n_C_prev, n_H_prev + 2 * self.p, n_W_prev + 2 * self.p)\n",
    "        )\n",
    "        \n",
    "        # Сначала посчитаем градиент по входу\n",
    "        W_rotated = self.W.detach().clone()\n",
    "        for c in range(n_C):\n",
    "            for old_c in range(n_C_prev):\n",
    "                W_rotated[c, old_c] = Conv.rotate180(self.W[c, old_c])\n",
    "        \n",
    "        for i in range(m):\n",
    "            dx = dX[i]\n",
    "            # Как показывается в последней статье, нужно добавить \n",
    "            # padding размера filter_size - 1 со всех сторон, а также \n",
    "            # расширить каждую строку/столбец на stride - 1\n",
    "            dz_transformed = torch.nn.functional.pad(\n",
    "                self._dilate(dZ[i]), \n",
    "                (self.f - 1, self.f - 1, self.f - 1, self.f - 1), \n",
    "                mode='constant', value=0\n",
    "            )\n",
    "            for h in range(n_H_prev):\n",
    "                for w in range(n_W_prev):\n",
    "                    window = dz_transformed[:, h : h + self.f, w : w + self.f]\n",
    "\n",
    "                    for c in range(n_C_prev):\n",
    "                        dx[c, h, w] = Conv.single_conv(\n",
    "                            window, W_rotated[:, c, :, :], 0.\n",
    "                        )\n",
    "        # Так как мы могли делать padding при forward pass, нужно сейчас\n",
    "        # вернуть к прежним размерам градиент\n",
    "        if self.p > 0:\n",
    "            dX = dX[:, :, self.p : -self.p, self.p : -self.p]\n",
    "        assert dX.shape == self.cache['input'].shape\n",
    "        \n",
    "        # Теперь посчитаем градиенты по параметрам\n",
    "        layer_input_padded = torch.nn.functional.pad(\n",
    "            self.cache['input'], (self.p, self.p, self.p, self.p), \n",
    "            mode='constant', value=0\n",
    "        )\n",
    "        for i in range(m):\n",
    "            x = self.cache['input'][i]\n",
    "            dz_transformed = self._dilate(dZ[i])\n",
    "            for h in range(self.f):\n",
    "                for w in range(self.f):\n",
    "                    # Выбираем конкретный двумерный фильтр\n",
    "                    for c in range(n_C_prev):\n",
    "                        window = x[c, h : h + self.f, w : w + self.f]\n",
    "                        for new_c in range(n_C):\n",
    "                            # Теперь по формуле из ссылки на вторую часть \n",
    "                            # последней статьи посчитаем значение каждого \n",
    "                            # элемента в двумерном фильтре\n",
    "                            self.dW[new_c, c, h, w] = Conv.single_conv(\n",
    "                                window, dz_transformed[new_c], 0.\n",
    "                            )\n",
    "                            self.db[new_c] += dz_transformed[new_c].sum()\n",
    "        \n",
    "        return dX, self.dW, self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade385",
   "metadata": {},
   "source": [
    "**Замечание.** Дальше идут слои из первого задания, поэтому их просто скопируем оттуда ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15f6aff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:29:52.762507Z",
     "start_time": "2021-05-06T20:29:52.749429Z"
    }
   },
   "outputs": [],
   "source": [
    "class Fc():  # TODO: в чём смысл ставить круглые скобки???\n",
    "    def __init__(self, row, column):  # bias всегда есть\n",
    "        self.row = row\n",
    "        self.col = column\n",
    "\n",
    "        self.W = torch_model_params[2].t()\n",
    "        self.dW = torch.zeros(self.W.shape)\n",
    "        \n",
    "        self.b = torch_model_params[3]\n",
    "        self.db = torch.zeros(self.b.shape)\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, fc):\n",
    "        self.cache['input'] = fc\n",
    "        \n",
    "        self.cache['output'] = torch.matmul(fc, self.W)\n",
    "        self.cache['output'] += self.b\n",
    "\n",
    "        return self.cache['output']\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # не забываем, что для эмуляции CrossEntropyLoss(reduction='mean') \n",
    "        # нужно делить self.dW и self.db на размер пакета\n",
    "        \n",
    "        # Не знаю, к чему комментарий выше, ведь мы легко это учтём в самом\n",
    "        # классе CrossEntropy\n",
    "        self.dW = torch.matmul(self.cache['input'].t(), dZ)\n",
    "        self.db = torch.sum(dZ, dim=0)\n",
    "        \n",
    "        return torch.matmul(dZ, self.W.t()), self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8450bb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:35:23.183016Z",
     "start_time": "2021-05-06T20:35:23.172865Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD():  # TODO: в чём смысл ставить круглые скобки???\n",
    "    \"\"\"Простой SGD без моментов и регуляризаций.\"\"\"\n",
    "    \n",
    "    def __init__(self, lr, params):\n",
    "        \"\"\"        \n",
    "        Параметры\n",
    "        ---------\n",
    "        lr : float\n",
    "            Шаг оптимизатора.\n",
    "        params : Dict{str : torch.tensor}\n",
    "            Словарь параметров модели. Ключи словаря --- названия параметров.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        new_params = self.params\n",
    "#         print(grads)\n",
    "        \n",
    "        new_params['W1'] -= self.lr * grads['dW1']\n",
    "        new_params['b1'] -= self.lr * grads['db1']\n",
    "        new_params['W2'] -= self.lr * grads['dW2']\n",
    "        new_params['b2'] -= self.lr * grads['db2']\n",
    "        \n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ecfb6823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:33:28.650917Z",
     "start_time": "2021-05-06T20:33:28.646051Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU():  # TODO: в чём смысл ставить круглые скобки???\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return torch.maximum(X, torch.tensor(0.))\n",
    "    \n",
    "    def backward(self, new_deltaL):\n",
    "        return torch.mul(new_deltaL, self.input > 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7a19512c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:33:29.136497Z",
     "start_time": "2021-05-06T20:33:29.125492Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax():  # TODO: в чём смысл ставить круглые скобки???\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Нормализуем для численной устойчивости, а потом возводим в exp\n",
    "        self.output = torch.exp(X - X.max(dim=1, keepdim=True).values)\n",
    "        self.output /= self.output.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, dZ):\n",
    "        grad_input = torch.zeros(size=self.output.shape)\n",
    "\n",
    "        for i in range(self.output.shape[0]):\n",
    "            softmax_i = self.output[i, :].unsqueeze(1)\n",
    "            partial_softmax = -torch.matmul(softmax_i, softmax_i.t()) + \\\n",
    "            torch.diag(softmax_i.squeeze())\n",
    "            for j in range(self.output.shape[1]):\n",
    "                grad_input[i, j] = torch.dot(dZ[i, :], partial_softmax[:, j])\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a0670f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:33:29.270465Z",
     "start_time": "2021-05-06T20:33:29.231177Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogSoftmax:\n",
    "    \"\"\"Осуществляет log(softmax)-преобразование.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X.detach().clone()\n",
    "        \n",
    "        # Нормализуем для численной устойчивости\n",
    "        self.output = X - X.max(axis=1, keepdim=True).values\n",
    "        self.output = self.output - torch.log(torch.sum(torch.exp(self.output), \n",
    "                                                        dim=1, keepdim=True))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # Нормализуем для численной устойчивости, а потом уже возводим в exp\n",
    "        exp_module_input = torch.exp(\n",
    "            self.input - self.input.max(axis=1, keepdim=True).values\n",
    "        )\n",
    "        softmax = exp_module_input / torch.sum(exp_module_input, dim=1, \n",
    "                                               keepdim=True)\n",
    "\n",
    "        grad_input = dZ - torch.mul(softmax, torch.sum(dZ, dim=1, keepdim=True))\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "16d4989a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:33:29.376235Z",
     "start_time": "2021-05-06T20:33:29.362750Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "\n",
    "    def __init__(self):\n",
    "#         self.sm = Softmax() # Softmax не нужен в последнем слое модели сети, если вызывать в расчёте кросс-энтропии\n",
    "        self.log_sm = LogSoftmax()\n",
    "    \n",
    "    # TODO: беее, а где же общие интерфейсы и соответствие им?((\n",
    "    def get(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        Считает лосс и градиент лосса.\n",
    "        \n",
    "        Параметры\n",
    "        ---------\n",
    "        y_pred : torch.tensor, shape = (batch_size, n_classes)\n",
    "            Предсказанные вероятности принадлежности классам.\n",
    "        y : torch.tensor, shape = (batch_size, n_classes)\n",
    "            One-hot encoding истинных меток классов.\n",
    "        \"\"\"\n",
    "        \n",
    "        # forward\n",
    "        log_probs = self.log_sm.forward(y_pred)\n",
    "        # TODO: нужно на число элементов в y_true???????????\n",
    "        cross_entropy = -torch.sum(torch.mul(log_probs, y)) / y.shape[0]\n",
    "        \n",
    "        # backward\n",
    "        dL = self.log_sm.backward(-y / y.shape[0])\n",
    "\n",
    "        return cross_entropy, dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ed331084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:41:40.320439Z",
     "start_time": "2021-05-06T20:41:40.313170Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b894b-cf32-40cf-84e0-d172f23436fe",
   "metadata": {},
   "source": [
    "Реализовать модель, идентичную **TorchGradientModel**. \n",
    "\n",
    "Если есть желание переиспользовать код из первого модуля заданий по линейным слоям, то можно реализовать модель, пользуясь предыдущими абстрактными классами и импортировать их здесь, переписав код для проверки результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9d7233e2-c485-45f0-b33f-3e3de37741c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:42:04.497229Z",
     "start_time": "2021-05-06T20:42:04.476747Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomModel:\n",
    "    \n",
    "    def __init__(self, input_size=5, num_classes=2):\n",
    "        self.conv1 = Conv(nb_filters=1, filter_size=input_size, nb_channels=4,\n",
    "                          padding=0, stride=1)\n",
    "        self.act1 = ReLU()\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Fc(4 * 1 * 1, num_classes)\n",
    "        \n",
    "        self.layers = [self.conv1, self.fc1]\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.conv1.forward(x)\n",
    "#         print(x.shape)\n",
    "        x = self.act1.forward(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = self.flatten.forward(x)\n",
    "#         print(x.shape)\n",
    "        x = self.fc1.forward(x)\n",
    "#         print(x.shape)\n",
    "#         print('============================================================')\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        dX, dW2, db2 = self.fc1.backward(deltaL)\n",
    "#         print(dX.shape)\n",
    "        dX = self.flatten.backward(dX)\n",
    "#         print(dX.shape)\n",
    "        dX = self.act1.backward(dX)\n",
    "#         print(dX.shape)\n",
    "        dX, dW1, db1 = self.conv1.backward(dX)\n",
    "#         print(dX.shape)\n",
    "        \n",
    "        grads = { \n",
    "                'dW1': dW1, 'db1': db1,\n",
    "                'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W\n",
    "            params['b' + str(i+1)] = layer.b\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W = params['W'+ str(i+1)]\n",
    "            layer.b = params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "afc89670-2027-4321-abbb-a3f5365054ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:42:04.787801Z",
     "start_time": "2021-05-06T20:42:04.778010Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_custom_model(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel(input_size=X.shape[-1], num_classes=num_classes)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr=learning_rate, params=params)      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        print('Parameters')\n",
    "        for name, val in params.items():\n",
    "            print(name, val)\n",
    "        print('=============================================================')\n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        # TODO: зачем такой функционал... параметры ведь по ссылке передаются\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = torch.sum(torch.argmax(y_pred, axis=1) == labels).item() / X.shape[0]\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba60e2ae-3472-4d16-ad2f-076dd35f839e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:42:05.554604Z",
     "start_time": "2021-05-06T20:42:05.402580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.33it/s, acc=1, loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "W1 tensor([[[[ 9.3454e-03, -1.4351e-01, -9.4997e-02,  3.6663e-02, -2.4648e-01],\n",
      "          [ 5.1845e-01,  2.9738e-01,  3.4085e-01,  1.6748e-01,  1.5756e-02],\n",
      "          [ 9.0673e-01,  7.0271e-01,  5.6802e-01,  2.1916e-01,  8.4649e-02],\n",
      "          [-9.8270e-01, -7.5855e-01, -5.9145e-01, -2.1893e-01, -1.3465e-01],\n",
      "          [-5.6669e-01, -5.1456e-01, -3.2885e-01, -2.8315e-01, -1.6253e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5763e-03,  1.1923e-01,  1.0871e-01, -1.9551e-01,  1.2398e-01],\n",
      "          [ 5.5873e-02,  1.8971e-01,  1.3201e-01, -1.8223e-01, -1.9016e-01],\n",
      "          [-9.6466e-02,  1.7562e-01, -3.3314e-02,  8.5592e-02, -9.2942e-02],\n",
      "          [ 1.9624e-01, -8.4620e-02,  1.4998e-01,  2.3683e-03, -1.0536e-01],\n",
      "          [ 1.0280e-01, -1.0616e-01,  5.8821e-02, -5.7751e-02, -2.1927e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6596e-01, -7.9842e-02,  1.2432e-01, -3.2547e-02,  2.1556e-01],\n",
      "          [ 4.8393e-02, -1.0445e-01, -2.0065e-01,  8.2563e-02,  5.0768e-02],\n",
      "          [-3.6837e-01, -6.4685e-02, -1.7599e-02,  9.3934e-02, -1.3998e-01],\n",
      "          [ 2.3803e-01,  2.1732e-01,  1.9510e-01, -4.1189e-02, -8.6627e-02],\n",
      "          [ 2.4704e-01,  6.3697e-02,  1.8064e-01,  6.6699e-02,  9.3318e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0813e-01,  1.1054e-01,  1.3764e-01, -1.4605e-01,  1.4051e-01],\n",
      "          [-4.1345e-01, -3.8267e-01, -1.5153e-01, -1.2971e-01,  8.9912e-02],\n",
      "          [-6.9862e-01, -6.5214e-01, -1.9332e-01, -2.3306e-01,  4.4555e-02],\n",
      "          [ 7.2476e-01,  3.8920e-01,  4.4456e-01,  3.1763e-01,  8.8136e-04],\n",
      "          [ 2.4209e-01,  3.3778e-01,  2.3801e-01,  4.9719e-02,  6.8534e-03]]]])\n",
      "b1 tensor([ -2.3437,  -3.5457,  -0.4511, -14.7133])\n",
      "W2 tensor([[-1.3330,  1.3525],\n",
      "        [-0.2247, -0.0969],\n",
      "        [ 0.4435,  0.1973],\n",
      "        [-0.9959,  1.3954]])\n",
      "b2 tensor([ 0.3909, -0.2485])\n",
      "=============================================================\n",
      "{'dW1': tensor([[[[ 7.9221e-06,  4.6698e-06,  4.7532e-06,  4.8366e-06,  4.9200e-06],\n",
      "          [-3.3690e-05, -2.8619e-05, -2.0214e-05, -1.1808e-05, -3.4023e-06],\n",
      "          [-7.1966e-05, -5.8573e-05, -4.1845e-05, -2.5117e-05, -8.3890e-06],\n",
      "          [ 8.1555e-05,  6.6579e-05,  4.9934e-05,  3.3289e-05,  1.6645e-05],\n",
      "          [ 4.1612e-05,  3.3289e-05,  2.4967e-05,  1.6645e-05,  8.3223e-06]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]]), 'db1': tensor([-0.0005,  0.0000,  0.0000,  0.0000]), 'dW2': tensor([[ 3.4440e-05, -3.4463e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00]]), 'db2': tensor([-0.1727,  0.1727])}\n",
      "Parameters\n",
      "W1 tensor([[[[ 9.3375e-03, -1.4352e-01, -9.5001e-02,  3.6658e-02, -2.4649e-01],\n",
      "          [ 5.1848e-01,  2.9741e-01,  3.4087e-01,  1.6749e-01,  1.5759e-02],\n",
      "          [ 9.0680e-01,  7.0277e-01,  5.6806e-01,  2.1919e-01,  8.4658e-02],\n",
      "          [-9.8278e-01, -7.5861e-01, -5.9150e-01, -2.1897e-01, -1.3466e-01],\n",
      "          [-5.6673e-01, -5.1459e-01, -3.2887e-01, -2.8317e-01, -1.6254e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5763e-03,  1.1923e-01,  1.0871e-01, -1.9551e-01,  1.2398e-01],\n",
      "          [ 5.5873e-02,  1.8971e-01,  1.3201e-01, -1.8223e-01, -1.9016e-01],\n",
      "          [-9.6466e-02,  1.7562e-01, -3.3314e-02,  8.5592e-02, -9.2942e-02],\n",
      "          [ 1.9624e-01, -8.4620e-02,  1.4998e-01,  2.3683e-03, -1.0536e-01],\n",
      "          [ 1.0280e-01, -1.0616e-01,  5.8821e-02, -5.7751e-02, -2.1927e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6596e-01, -7.9842e-02,  1.2432e-01, -3.2547e-02,  2.1556e-01],\n",
      "          [ 4.8393e-02, -1.0445e-01, -2.0065e-01,  8.2563e-02,  5.0768e-02],\n",
      "          [-3.6837e-01, -6.4685e-02, -1.7599e-02,  9.3934e-02, -1.3998e-01],\n",
      "          [ 2.3803e-01,  2.1732e-01,  1.9510e-01, -4.1189e-02, -8.6627e-02],\n",
      "          [ 2.4704e-01,  6.3697e-02,  1.8064e-01,  6.6699e-02,  9.3318e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0813e-01,  1.1054e-01,  1.3764e-01, -1.4605e-01,  1.4051e-01],\n",
      "          [-4.1345e-01, -3.8267e-01, -1.5153e-01, -1.2971e-01,  8.9912e-02],\n",
      "          [-6.9862e-01, -6.5214e-01, -1.9332e-01, -2.3306e-01,  4.4555e-02],\n",
      "          [ 7.2476e-01,  3.8920e-01,  4.4456e-01,  3.1763e-01,  8.8136e-04],\n",
      "          [ 2.4209e-01,  3.3778e-01,  2.3801e-01,  4.9719e-02,  6.8534e-03]]]])\n",
      "b1 tensor([ -2.3431,  -3.5457,  -0.4511, -14.7133])\n",
      "W2 tensor([[-1.3330,  1.3526],\n",
      "        [-0.2247, -0.0969],\n",
      "        [ 0.4435,  0.1973],\n",
      "        [-0.9959,  1.3954]])\n",
      "b2 tensor([ 0.5635, -0.4212])\n",
      "=============================================================\n",
      "{'dW1': tensor([[[[ 1.1158e-05,  6.5773e-06,  6.6948e-06,  6.8122e-06,  6.9297e-06],\n",
      "          [-4.7451e-05, -4.0310e-05, -2.8470e-05, -1.6631e-05, -4.7920e-06],\n",
      "          [-1.0136e-04, -8.2498e-05, -5.8937e-05, -3.5377e-05, -1.1816e-05],\n",
      "          [ 1.1487e-04,  9.3774e-05,  7.0330e-05,  4.6887e-05,  2.3443e-05],\n",
      "          [ 5.8609e-05,  4.6887e-05,  3.5165e-05,  2.3443e-05,  1.1722e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]]), 'db1': tensor([-0.0013,  0.0000,  0.0000,  0.0000]), 'dW2': tensor([[ 4.8502e-05, -4.8568e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00]]), 'db2': tensor([-0.1360,  0.1360])}\n",
      "Parameters\n",
      "W1 tensor([[[[ 9.3263e-03, -1.4352e-01, -9.5008e-02,  3.6651e-02, -2.4650e-01],\n",
      "          [ 5.1853e-01,  2.9745e-01,  3.4090e-01,  1.6751e-01,  1.5764e-02],\n",
      "          [ 9.0690e-01,  7.0285e-01,  5.6812e-01,  2.1922e-01,  8.4670e-02],\n",
      "          [-9.8289e-01, -7.5871e-01, -5.9157e-01, -2.1901e-01, -1.3469e-01],\n",
      "          [-5.6679e-01, -5.1464e-01, -3.2891e-01, -2.8319e-01, -1.6255e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5763e-03,  1.1923e-01,  1.0871e-01, -1.9551e-01,  1.2398e-01],\n",
      "          [ 5.5873e-02,  1.8971e-01,  1.3201e-01, -1.8223e-01, -1.9016e-01],\n",
      "          [-9.6466e-02,  1.7562e-01, -3.3314e-02,  8.5592e-02, -9.2942e-02],\n",
      "          [ 1.9624e-01, -8.4620e-02,  1.4998e-01,  2.3683e-03, -1.0536e-01],\n",
      "          [ 1.0280e-01, -1.0616e-01,  5.8821e-02, -5.7751e-02, -2.1927e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6596e-01, -7.9842e-02,  1.2432e-01, -3.2547e-02,  2.1556e-01],\n",
      "          [ 4.8393e-02, -1.0445e-01, -2.0065e-01,  8.2563e-02,  5.0768e-02],\n",
      "          [-3.6837e-01, -6.4685e-02, -1.7599e-02,  9.3934e-02, -1.3998e-01],\n",
      "          [ 2.3803e-01,  2.1732e-01,  1.9510e-01, -4.1189e-02, -8.6627e-02],\n",
      "          [ 2.4704e-01,  6.3697e-02,  1.8064e-01,  6.6699e-02,  9.3318e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0813e-01,  1.1054e-01,  1.3764e-01, -1.4605e-01,  1.4051e-01],\n",
      "          [-4.1345e-01, -3.8267e-01, -1.5153e-01, -1.2971e-01,  8.9912e-02],\n",
      "          [-6.9862e-01, -6.5214e-01, -1.9332e-01, -2.3306e-01,  4.4555e-02],\n",
      "          [ 7.2476e-01,  3.8920e-01,  4.4456e-01,  3.1763e-01,  8.8136e-04],\n",
      "          [ 2.4209e-01,  3.3778e-01,  2.3801e-01,  4.9719e-02,  6.8534e-03]]]])\n",
      "b1 tensor([ -2.3418,  -3.5457,  -0.4511, -14.7133])\n",
      "W2 tensor([[-1.3331,  1.3526],\n",
      "        [-0.2247, -0.0969],\n",
      "        [ 0.4435,  0.1973],\n",
      "        [-0.9959,  1.3954]])\n",
      "b2 tensor([ 0.6995, -0.5572])\n",
      "=============================================================\n",
      "{'dW1': tensor([[[[ 1.4555e-05,  8.5800e-06,  8.7333e-06,  8.8865e-06,  9.0397e-06],\n",
      "          [-6.1899e-05, -5.2583e-05, -3.7139e-05, -2.1695e-05, -6.2512e-06],\n",
      "          [-1.3222e-04, -1.0762e-04, -7.6883e-05, -4.6148e-05, -1.5413e-05],\n",
      "          [ 1.4984e-04,  1.2233e-04,  9.1745e-05,  6.1163e-05,  3.0582e-05],\n",
      "          [ 7.6454e-05,  6.1163e-05,  4.5873e-05,  3.0582e-05,  1.5291e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 11241 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_custom_model(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050c96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:32:47.541080Z",
     "start_time": "2021-05-06T20:32:47.264Z"
    }
   },
   "outputs": [],
   "source": [
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
    "          [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
    "          [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
    "          [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
    "          [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
    "          [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
    "          [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
    "          [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
    "          [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
    "\n",
    "\n",
    "        [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
    "          [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
    "          [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
    "          [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
    "          [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
    "          [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
    "          [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
    "          [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
    "          [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]]], requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.1832, -0.0370,  0.1128,  0.0866], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.3232, -0.4252,  0.4799,  0.0261],\n",
    "        [ 0.3427,  0.1036,  0.1608,  0.3735]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4741, -0.3318], requires_grad=True))\n",
    "==============================================================\n",
    "\n",
    "100%|██████████| 3/3 [00:00<00:00,  3.54it/s, accuracy=1, loss=0.0179]\n",
    "\n",
    "Parameters\n",
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 0.0790, -0.1124, -0.4897,  0.0344, -0.2124],\n",
    "          [ 0.0958, -0.0652, -0.3758, -0.0051, -0.0255],\n",
    "          [ 0.5708,  0.5599,  0.1697,  0.4721,  0.4777],\n",
    "          [-0.0305, -0.0040, -0.4783,  0.1507,  0.0381],\n",
    "          [-0.0986, -0.1803, -0.4845, -0.1104, -0.0882]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0814,  0.1911, -0.1584, -0.1517,  0.1959],\n",
    "          [ 0.1277,  0.2616, -0.1562, -0.1174, -0.1183],\n",
    "          [-0.0246,  0.2475, -0.3215,  0.1575, -0.0211],\n",
    "          [ 0.2681, -0.0198, -0.1382,  0.0742, -0.0335],\n",
    "          [ 0.1747, -0.0624, -0.2083,  0.0141,  0.0499]]],\n",
    "\n",
    "\n",
    "        [[[-0.1807, -0.0838,  0.3246, -0.0201,  0.2108],\n",
    "          [ 0.1719,  0.0022,  0.0953,  0.1376,  0.0625],\n",
    "          [-0.3763, -0.1337,  0.0751, -0.0862, -0.3480],\n",
    "          [-0.0269,  0.0119,  0.2580, -0.1402, -0.1304],\n",
    "          [ 0.1204, -0.0184,  0.3138,  0.0230,  0.0772]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0314,  0.0601, -0.1361, -0.2162,  0.0881],\n",
    "          [-0.1532, -0.1635, -0.2369, -0.0513,  0.1170],\n",
    "          [ 0.1530,  0.1091,  0.1960,  0.2573,  0.3693],\n",
    "          [ 0.0578, -0.1672, -0.2089,  0.0355, -0.1465],\n",
    "          [-0.1075,  0.0372, -0.1994, -0.0976, -0.0731]]]], requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.2302, -0.1774,  0.0902,  0.1112], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.3022, -0.2247,  0.4435,  0.2730],\n",
    "        [ 0.3217, -0.0969,  0.1973,  0.1266]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4034, -0.2611], requires_grad=True))\n",
    "==============================================================\n",
    "Parameters\n",
    "('conv1.weight', Parameter containing:\n",
    "tensor([[[[ 6.7224e-02, -1.2410e-01, -5.0147e-01,  2.2663e-02, -2.2412e-01],\n",
    "          [ 8.4114e-02, -7.6974e-02, -3.8749e-01, -1.6871e-02, -3.4586e-02],\n",
    "          [ 6.1396e-01,  6.0695e-01,  2.1672e-01,  5.1914e-01,  5.1817e-01],\n",
    "          [-4.0968e-02, -1.5686e-02, -4.9003e-01,  1.3900e-01,  2.6409e-02],\n",
    "          [-1.1031e-01, -1.9200e-01, -4.9622e-01, -1.2210e-01, -9.9919e-02]]],\n",
    "\n",
    "\n",
    "        [[[ 8.1444e-02,  1.9109e-01, -1.5841e-01, -1.5174e-01,  1.9585e-01],\n",
    "          [ 1.2774e-01,  2.6158e-01, -1.5618e-01, -1.1738e-01, -1.1829e-01],\n",
    "          [-2.4599e-02,  2.4749e-01, -3.2151e-01,  1.5746e-01, -2.1074e-02],\n",
    "          [ 2.6811e-01, -1.9776e-02, -1.3821e-01,  7.4236e-02, -3.3496e-02],\n",
    "          [ 1.7467e-01, -6.2394e-02, -2.0830e-01,  1.4116e-02,  4.9941e-02]]],\n",
    "\n",
    "\n",
    "        [[[-1.9430e-01, -9.7377e-02,  3.7490e-01, -2.8138e-02,  1.9719e-01],\n",
    "          [ 1.5830e-01, -1.1385e-02,  1.4975e-01,  1.2540e-01,  4.8969e-02],\n",
    "          [-3.8983e-01, -1.4726e-01,  1.2952e-01, -9.9788e-02, -3.6157e-01],\n",
    "          [-4.0482e-02, -2.7019e-04,  3.1246e-01, -1.5380e-01, -1.4394e-01],\n",
    "          [ 1.0678e-01, -2.6421e-02,  3.6407e-01,  9.3816e-03,  6.3649e-02]]],\n",
    "\n",
    "\n",
    "        [[[ 3.4158e-02,  6.2900e-02, -1.3335e-01, -2.1350e-01,  9.0850e-02],\n",
    "          [-1.5044e-01, -1.6072e-01, -2.3417e-01, -4.8516e-02,  1.1910e-01],\n",
    "          [ 1.4289e-01,  9.8093e-02,  1.8493e-01,  2.4623e-01,  3.5975e-01],\n",
    "          [ 6.0202e-02, -1.6441e-01, -2.0615e-01,  3.8219e-02, -1.4373e-01],\n",
    "          [-1.0471e-01,  3.9919e-02, -1.9666e-01, -9.4895e-02, -7.0363e-02]]]],\n",
    "       requires_grad=True))\n",
    "('conv1.bias', Parameter containing:\n",
    "tensor([ 0.2531, -0.1774,  0.1167,  0.1058], requires_grad=True))\n",
    "('fc1.weight', Parameter containing:\n",
    "tensor([[-0.5160, -0.2247,  0.7183,  0.1626],\n",
    "        [ 0.5355, -0.0969, -0.0775,  0.2370]], requires_grad=True))\n",
    "('fc1.bias', Parameter containing:\n",
    "tensor([ 0.4744, -0.3320], requires_grad=True))\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "591454a2-a616-49c7-9dd1-18d323492924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:42:17.627371Z",
     "start_time": "2021-05-06T20:42:16.654105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 1 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e570f1c4-26d0-4d56-9b8c-79d70773f7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:42:17.649068Z",
     "start_time": "2021-05-06T20:42:17.632700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задание 1 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной\n",
      "Суммарная ошибка = 0.8323\n"
     ]
    }
   ],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 1 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 1 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task1 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e52a3a-fde2-401b-9517-b41492d445b9",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7295ab-2cf9-4eac-b525-62f3777bd4fa",
   "metadata": {},
   "source": [
    "Требуется реализовать классы пакетной нормализации **BatchNorm2d** и слоя выборки усреднением **AvgPool**. Реализовать класс модели, идентичной эталонной TorchGradientModel2\n",
    "\n",
    "Эталонная модель **TorchGradientModel2**, состоить из:\n",
    "- Сверточный слой с 4 фильтрами размера $3\\times3$;\n",
    "- Пакетная нормализация;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Выборки усреднением с ашгом 2 и размером окна 2\n",
    "- Линейный слой с переменных количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82edce-6429-4fd5-957a-d373e8ed46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchGradientModel2(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=0, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * int((input_size - 2) / 2) * int((input_size - 2) / 2), num_classes)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b11589-27fa-4201-ab3f-079ba27c8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. При желании можете поэкспериментировать с другими, которые реализовали для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a7ef0-9f4d-440c-8a6c-eaa9f884eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d():\n",
    "    def __init__(self, num_channels, gamma=1, beta=0, eps=1e-20):\n",
    "        self.num_channels = num_channels\n",
    "        # Применяем стандартные название полей для обновления весов, чтобы не переписывать код модели и оптимизатора\n",
    "        self.W = torch.ones(num_channels)  # gamma\n",
    "        self.b = torch.zeros(num_channels)  # beta\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.dW = torch.zeros(num_channels)\n",
    "        self.db = torch.zeros(num_channels)\n",
    "\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, debug=True):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool():\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.f = filter_size\n",
    "        self.s = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямой проход для усредняющего пулинга.\n",
    "        \n",
    "        Аргументы\n",
    "        ---------\n",
    "        X : torch.tensor, shape = (m, n_C, n_H_prev, n_W_prev)\n",
    "        \n",
    "        Возвращает\n",
    "        ----------\n",
    "        torch.tensor, shape = (m, n_C, n_H, n_W)\n",
    "        \"\"\"\n",
    "        \n",
    "        m, n_C, n_H_prev, n_W_prev = X.shape\n",
    "        \n",
    "        # Посчитаем новый размеры по формуле из доков AvgPool2d\n",
    "        n_H = int(1 + (n_H_prev - self.f) / self.s)\n",
    "        n_W = int(1 + (n_W_prev - self.f) / self.s)\n",
    "        \n",
    "        Z = torch.zeros((m, n_C, n_H, n_W))\n",
    "        \n",
    "        for i in range(m):\n",
    "            cur_batch_elem = X[i]\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    # Определим границы текущего окна\n",
    "                    top_left_h = h * self.s\n",
    "                    top_left_w = w * self.s\n",
    "                    bottom_right_h = top_left_h + self.f\n",
    "                    bottom_right_w = top_left_w + self.f\n",
    "                    \n",
    "                    for c in range(n_C):\n",
    "                        window = cur_batch_elem[c, top_left_h:bottom_right_h, \n",
    "                                                top_left_w:bottom_right_w]\n",
    "                        Z[i, h, w, c] = torch.mean(window)\n",
    "                        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77309d-ad2e-46e8-9ffd-4e157006ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel2():\n",
    "\n",
    "    def __init__(self, input_size = 5, num_classes = 2):\n",
    "        # ВПИСАТЬ МОДУЛИ СЕТИ ЗДЕСЬ\n",
    "        \n",
    "        self.layers = [self.conv1, self.bn1, self.fc1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W\n",
    "            params['b' + str(i+1)] = layer.b\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W = params['W'+ str(i+1)]\n",
    "            layer.b = params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651e71c-2a12-45c6-8f05-32c23fca32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model2(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel2(input_size = X.shape[-1], num_classes=num_classes)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr = learning_rate, params = model.get_params())      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0 \n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item() / X.shape[0]\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7053a57-ae3b-4f05-a73a-7d8b28079479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_model2(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d4eb9-b1ed-4919-8d37-98f12cbae3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2684-a0a0-4025-8713-01e696c4827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 2 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 2 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cb689-abb1-458f-b851-046583c29220",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71114630-0f89-4b70-8c7c-cbe5091f0440",
   "metadata": {},
   "source": [
    "Обучить ранее реализованную сеть **CustomModel2** на реальных данных из выборки digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96e6f0-3d16-4656-aaf9-299c5cd423a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aa79a-730a-4b53-9a6d-7769e5ab164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_numbers():\n",
    "    # Загружаем выборку digits\n",
    "    digits = load_digits(n_class=10, return_X_y=False, as_frame=False)\n",
    "\n",
    "    # ПРОВЕСТИ НЕОБХОДИМЫЕ ПРЕОБРАЗОВАНИЯ\n",
    "    \n",
    "    return train_x, train_label, val_x, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e861b8-49e4-4800-872a-c8ac7ee4093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_label, val_x, val_label = get_data_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766a8b7-dc86-4572-baa0-ecf80dec5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "num_classes = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2(num_classes, input_size=8)\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. При желании можете поэкспериментировать с другими, которые реализовали для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(train_x[0:10]) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, train_label[0:10])\n",
    "    loss_history.loc[e, 'loss_train_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == train_label[0:10]).item()\n",
    "\n",
    "    train_acc /= train_x[0:10].shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123a514-c557-44f2-baed-77f6b4adf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, learning_rate, loss_history, train_x, train_label, val_x, val_label):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e12010-8508-4c62-bf6d-e26cb66c62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "train_model(epochs, learning_rate, loss_history, train_x[0:10], train_label[0:10], val_x[0:10], val_label[0:10])\n",
    "\n",
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603aeed-717d-4d6e-ad07-a652a284e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute(loss_history[['loss_train_custom']].to_numpy()-loss_history[['loss_train_pt']].to_numpy()).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 3 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 3 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a740226-459e-46c6-92ab-c38dac6c3468",
   "metadata": {},
   "source": [
    "### Необязательное задание\n",
    "Создать свою архитектуру свёрточной сети и обучиться на полной выборке digits, получив высокое качество классификации. Сравнить скорость и финальное качество обучения при применении разных алгоритмов обучения (SGD с моментом, Adam), регуляризации весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac56e4-981f-46b3-9ffd-54e63054f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
